{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Introduction","metadata":{}},{"cell_type":"markdown","source":"Previously I've shown how to create a [linear model and neural net from scratch](https://www.kaggle.com/code/jhoward/linear-model-and-neural-net-from-scratch), and used it to create a solid submission to Kaggle's [Titanic](https://www.kaggle.com/competitions/titanic/) competition. However, for *tabular* data (i.e data that looks like spreadsheet or database tables, such as the data for the Titanic competition) it's more common to see good results by using ensembles of decision trees, such as Random Forests and Gradient Boosting Machines.\n\nIn this notebook, we're going to learn all about Random Forests, by building one from scratch, and using it to submit to the Titanic competition! That might sound like a pretty big stretch, but I think you'll be surprised to discover how straightforward it actually is.\n\nWe'll start by importing the basic set of libraries we normally need for data science work, and setting numpy to use our display space more efficiently:","metadata":{}},{"cell_type":"code","source":"from fastai.imports import *\nnp.set_printoptions(linewidth=130)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-11-01T10:34:18.707598Z","iopub.execute_input":"2022-11-01T10:34:18.707871Z","iopub.status.idle":"2022-11-01T10:34:18.713015Z","shell.execute_reply.started":"2022-11-01T10:34:18.707819Z","shell.execute_reply":"2022-11-01T10:34:18.712151Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"## Data preprocessing","metadata":{}},{"cell_type":"markdown","source":"We'll create `DataFrame`s from the CSV files just like we did in the \"*linear model and neural net from scratch*\" notebook, and do much the same preprocessing (so go back and check that out if you're not already familiar with the dataset):","metadata":{}},{"cell_type":"code","source":"import os\niskaggle = os.environ.get('KAGGLE_KERNEL_RUN_TYPE', '')\n\nif iskaggle: path = Path('../input/titanic')\nelse:\n    import zipfile,kaggle\n    path = Path('titanic')\n    kaggle.api.competition_download_cli(str(path))\n    zipfile.ZipFile(f'{path}.zip').extractall(path)\n\ndf = pd.read_csv(path/'train.csv')\ntst_df = pd.read_csv(path/'test.csv')\nmodes = df.mode().iloc[0]","metadata":{"execution":{"iopub.status.busy":"2022-11-01T10:34:25.385840Z","iopub.execute_input":"2022-11-01T10:34:25.386137Z","iopub.status.idle":"2022-11-01T10:34:25.432355Z","shell.execute_reply.started":"2022-11-01T10:34:25.386100Z","shell.execute_reply":"2022-11-01T10:34:25.431552Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"modes","metadata":{"execution":{"iopub.status.busy":"2022-11-01T10:34:38.342542Z","iopub.execute_input":"2022-11-01T10:34:38.342820Z","iopub.status.idle":"2022-11-01T10:34:38.351216Z","shell.execute_reply.started":"2022-11-01T10:34:38.342787Z","shell.execute_reply":"2022-11-01T10:34:38.350467Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"PassengerId                      1\nSurvived                       0.0\nPclass                         3.0\nName           Abbing, Mr. Anthony\nSex                           male\nAge                           24.0\nSibSp                          0.0\nParch                          0.0\nTicket                        1601\nFare                          8.05\nCabin                      B96 B98\nEmbarked                         S\nName: 0, dtype: object"},"metadata":{}}]},{"cell_type":"markdown","source":"One difference with Random Forests however is that we don't generally have to create *dummy variables* like we did for non-numeric columns in the linear models and neural network. Instead, we can just convert those fields to *categorical variables*, which internally in Pandas makes a list of all the unique values in the column, and replaces each value with a number. The number is just an index for looking up the value in the list of all unique values.","metadata":{}},{"cell_type":"code","source":"def proc_data(df):\n    \n    # Fill in missing data in fare collumn\n    df['Fare'] = df.Fare.fillna(0)\n    df.fillna(modes, inplace=True)\n    \n    # log fare for more normal data distribution\n    df['LogFare'] = np.log1p(df['Fare'])\n    df['Embarked'] = pd.Categorical(df.Embarked)\n    df['Sex'] = pd.Categorical(df.Sex)\n\nproc_data(df)\nproc_data(tst_df)","metadata":{"execution":{"iopub.status.busy":"2022-11-01T10:35:32.688280Z","iopub.execute_input":"2022-11-01T10:35:32.689177Z","iopub.status.idle":"2022-11-01T10:35:32.722774Z","shell.execute_reply.started":"2022-11-01T10:35:32.689132Z","shell.execute_reply":"2022-11-01T10:35:32.721970Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2022-11-01T10:35:50.881761Z","iopub.execute_input":"2022-11-01T10:35:50.883008Z","iopub.status.idle":"2022-11-01T10:35:50.911994Z","shell.execute_reply.started":"2022-11-01T10:35:50.882959Z","shell.execute_reply":"2022-11-01T10:35:50.910792Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"   PassengerId  Survived  Pclass  \\\n0            1         0       3   \n1            2         1       1   \n2            3         1       3   \n3            4         1       1   \n4            5         0       3   \n\n                                                  Name     Sex   Age  SibSp  \\\n0                              Braund, Mr. Owen Harris    male  22.0      1   \n1  Cumings, Mrs. John Bradley (Florence Briggs Thayer)  female  38.0      1   \n2                               Heikkinen, Miss. Laina  female  26.0      0   \n3         Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n4                             Allen, Mr. William Henry    male  35.0      0   \n\n   Parch            Ticket     Fare    Cabin Embarked   LogFare  \n0      0         A/5 21171   7.2500  B96 B98        S  2.110213  \n1      0          PC 17599  71.2833      C85        C  4.280593  \n2      0  STON/O2. 3101282   7.9250  B96 B98        S  2.188856  \n3      0            113803  53.1000     C123        S  3.990834  \n4      0            373450   8.0500  B96 B98        S  2.202765  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>PassengerId</th>\n      <th>Survived</th>\n      <th>Pclass</th>\n      <th>Name</th>\n      <th>Sex</th>\n      <th>Age</th>\n      <th>SibSp</th>\n      <th>Parch</th>\n      <th>Ticket</th>\n      <th>Fare</th>\n      <th>Cabin</th>\n      <th>Embarked</th>\n      <th>LogFare</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>0</td>\n      <td>3</td>\n      <td>Braund, Mr. Owen Harris</td>\n      <td>male</td>\n      <td>22.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>A/5 21171</td>\n      <td>7.2500</td>\n      <td>B96 B98</td>\n      <td>S</td>\n      <td>2.110213</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>1</td>\n      <td>1</td>\n      <td>Cumings, Mrs. John Bradley (Florence Briggs Thayer)</td>\n      <td>female</td>\n      <td>38.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>PC 17599</td>\n      <td>71.2833</td>\n      <td>C85</td>\n      <td>C</td>\n      <td>4.280593</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>1</td>\n      <td>3</td>\n      <td>Heikkinen, Miss. Laina</td>\n      <td>female</td>\n      <td>26.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>STON/O2. 3101282</td>\n      <td>7.9250</td>\n      <td>B96 B98</td>\n      <td>S</td>\n      <td>2.188856</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>1</td>\n      <td>1</td>\n      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n      <td>female</td>\n      <td>35.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>113803</td>\n      <td>53.1000</td>\n      <td>C123</td>\n      <td>S</td>\n      <td>3.990834</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>0</td>\n      <td>3</td>\n      <td>Allen, Mr. William Henry</td>\n      <td>male</td>\n      <td>35.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>373450</td>\n      <td>8.0500</td>\n      <td>B96 B98</td>\n      <td>S</td>\n      <td>2.202765</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"tst_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-11-01T10:36:11.370097Z","iopub.execute_input":"2022-11-01T10:36:11.371184Z","iopub.status.idle":"2022-11-01T10:36:11.395036Z","shell.execute_reply.started":"2022-11-01T10:36:11.371137Z","shell.execute_reply":"2022-11-01T10:36:11.394176Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"   PassengerId  Pclass                                          Name     Sex  \\\n0          892       3                              Kelly, Mr. James    male   \n1          893       3              Wilkes, Mrs. James (Ellen Needs)  female   \n2          894       2                     Myles, Mr. Thomas Francis    male   \n3          895       3                              Wirz, Mr. Albert    male   \n4          896       3  Hirvonen, Mrs. Alexander (Helga E Lindqvist)  female   \n\n    Age  SibSp  Parch   Ticket     Fare    Cabin Embarked   LogFare  \n0  34.5      0      0   330911   7.8292  B96 B98        Q  2.178064  \n1  47.0      1      0   363272   7.0000  B96 B98        S  2.079442  \n2  62.0      0      0   240276   9.6875  B96 B98        Q  2.369075  \n3  27.0      0      0   315154   8.6625  B96 B98        S  2.268252  \n4  22.0      1      1  3101298  12.2875  B96 B98        S  2.586824  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>PassengerId</th>\n      <th>Pclass</th>\n      <th>Name</th>\n      <th>Sex</th>\n      <th>Age</th>\n      <th>SibSp</th>\n      <th>Parch</th>\n      <th>Ticket</th>\n      <th>Fare</th>\n      <th>Cabin</th>\n      <th>Embarked</th>\n      <th>LogFare</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>892</td>\n      <td>3</td>\n      <td>Kelly, Mr. James</td>\n      <td>male</td>\n      <td>34.5</td>\n      <td>0</td>\n      <td>0</td>\n      <td>330911</td>\n      <td>7.8292</td>\n      <td>B96 B98</td>\n      <td>Q</td>\n      <td>2.178064</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>893</td>\n      <td>3</td>\n      <td>Wilkes, Mrs. James (Ellen Needs)</td>\n      <td>female</td>\n      <td>47.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>363272</td>\n      <td>7.0000</td>\n      <td>B96 B98</td>\n      <td>S</td>\n      <td>2.079442</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>894</td>\n      <td>2</td>\n      <td>Myles, Mr. Thomas Francis</td>\n      <td>male</td>\n      <td>62.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>240276</td>\n      <td>9.6875</td>\n      <td>B96 B98</td>\n      <td>Q</td>\n      <td>2.369075</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>895</td>\n      <td>3</td>\n      <td>Wirz, Mr. Albert</td>\n      <td>male</td>\n      <td>27.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>315154</td>\n      <td>8.6625</td>\n      <td>B96 B98</td>\n      <td>S</td>\n      <td>2.268252</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>896</td>\n      <td>3</td>\n      <td>Hirvonen, Mrs. Alexander (Helga E Lindqvist)</td>\n      <td>female</td>\n      <td>22.0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>3101298</td>\n      <td>12.2875</td>\n      <td>B96 B98</td>\n      <td>S</td>\n      <td>2.586824</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"We'll make a list of the continuous, categorical, and dependent variables. Note that we no longer consider `Pclass` a categorical variable. That's because it's *ordered* (i.e 1st, 2nd, and 3rd class have an order), and decision trees, as we'll see, only care about order, not about absolute value.","metadata":{}},{"cell_type":"code","source":"cats=[\"Sex\",\"Embarked\"]\nconts=['Age', 'SibSp', 'Parch', 'LogFare',\"Pclass\"]\ndep=\"Survived\"","metadata":{"execution":{"iopub.status.busy":"2022-11-01T10:36:42.562347Z","iopub.execute_input":"2022-11-01T10:36:42.562943Z","iopub.status.idle":"2022-11-01T10:36:42.567055Z","shell.execute_reply.started":"2022-11-01T10:36:42.562902Z","shell.execute_reply":"2022-11-01T10:36:42.566166Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"Even although we've made the `cats` columns categorical, they are still shown by Pandas as their original values:","metadata":{}},{"cell_type":"code","source":"df.Sex.head()","metadata":{"execution":{"iopub.status.busy":"2022-11-01T10:36:47.403349Z","iopub.execute_input":"2022-11-01T10:36:47.403974Z","iopub.status.idle":"2022-11-01T10:36:47.412027Z","shell.execute_reply.started":"2022-11-01T10:36:47.403933Z","shell.execute_reply":"2022-11-01T10:36:47.411013Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"0      male\n1    female\n2    female\n3    female\n4      male\nName: Sex, dtype: category\nCategories (2, object): ['female', 'male']"},"metadata":{}}]},{"cell_type":"markdown","source":"However behind the scenes they're now stored as integers, with indices that are looked up in the `Categories` list shown in the output above. We can view the stored values by looking in the `cat.codes` attribute:","metadata":{}},{"cell_type":"code","source":"df.Sex.cat.codes.head()","metadata":{"execution":{"iopub.status.busy":"2022-11-01T10:36:50.639459Z","iopub.execute_input":"2022-11-01T10:36:50.640069Z","iopub.status.idle":"2022-11-01T10:36:50.646693Z","shell.execute_reply.started":"2022-11-01T10:36:50.640028Z","shell.execute_reply":"2022-11-01T10:36:50.645918Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"0    1\n1    0\n2    0\n3    0\n4    1\ndtype: int8"},"metadata":{}}]},{"cell_type":"markdown","source":"## Binary splits","metadata":{}},{"cell_type":"markdown","source":"Before we create a Random Forest or Gradient Boosting Machine, we'll first need to learn how to create a *decision tree*, from which both of these models are built.\n\nAnd to create a decision tree, we'll first need to create a *binary split*, since that's what a decision tree is built from.\n\nA binary split is where all rows are placed into one of two groups, based on whether they're above or below some threshold of some column. For example, we could split the rows of our dataset into males and females, by using the threshold `0.5` and the column `Sex` (since the values in the column are `0` for `female` and `1` for `male`). We can use a plot to see how that would split up our data -- we'll use the [Seaborn](https://seaborn.pydata.org/) library, which is a layer on top of [matplotlib](https://matplotlib.org/) that makes some useful charts easier to create, and more aesthetically pleasing by default:","metadata":{}},{"cell_type":"code","source":"import seaborn as sns\n\nfig,axs = plt.subplots(1,2, figsize=(11,5))\nsns.barplot(data=df, y=dep, x=\"Sex\", ax=axs[0]).set(title=\"Survival rate\")\nsns.countplot(data=df, x=\"Sex\", ax=axs[1]).set(title=\"Histogram\");","metadata":{"execution":{"iopub.status.busy":"2022-11-01T10:37:17.875197Z","iopub.execute_input":"2022-11-01T10:37:17.876464Z","iopub.status.idle":"2022-11-01T10:37:18.359147Z","shell.execute_reply.started":"2022-11-01T10:37:17.876417Z","shell.execute_reply":"2022-11-01T10:37:18.358460Z"},"trusted":true},"execution_count":13,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 792x360 with 2 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"markdown","source":"Here we see that (on the left) if we split the data into males and females, we'd have groups that have very different survival rates: >70% for females, and <20% for males. We can also see (on the right) that the split would be reasonably even, with over 300 passengers (out of around 900) in each group.\n\nWe could create a very simple \"model\" which simply says that all females survive, and no males do. To do so, we better first split our data into a training and validation set, to see how accurate this approach turns out to be:","metadata":{}},{"cell_type":"code","source":"from numpy import random\nfrom sklearn.model_selection import train_test_split\n\nrandom.seed(42)\n# Split training data\ntrn_df,val_df = train_test_split(df, test_size=0.25)\n# Change Sex and Embarked columns in both train and validation sets\ntrn_df[cats] = trn_df[cats].apply(lambda x: x.cat.codes)\nval_df[cats] = val_df[cats].apply(lambda x: x.cat.codes)","metadata":{"execution":{"iopub.status.busy":"2022-11-01T10:38:29.671966Z","iopub.execute_input":"2022-11-01T10:38:29.672282Z","iopub.status.idle":"2022-11-01T10:38:29.686694Z","shell.execute_reply.started":"2022-11-01T10:38:29.672243Z","shell.execute_reply":"2022-11-01T10:38:29.685733Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"trn_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-11-01T10:38:51.119219Z","iopub.execute_input":"2022-11-01T10:38:51.119570Z","iopub.status.idle":"2022-11-01T10:38:51.144136Z","shell.execute_reply.started":"2022-11-01T10:38:51.119530Z","shell.execute_reply":"2022-11-01T10:38:51.143339Z"},"trusted":true},"execution_count":19,"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"     PassengerId  Survived  Pclass                             Name  Sex  \\\n298          299         1       1            Saalfeld, Mr. Adolphe    1   \n884          885         0       3           Sutehall, Mr. Henry Jr    1   \n247          248         1       2  Hamalainen, Mrs. William (Anna)    0   \n478          479         0       3        Karlsson, Mr. Nils August    1   \n305          306         1       1   Allison, Master. Hudson Trevor    1   \n\n       Age  SibSp  Parch           Ticket      Fare    Cabin  Embarked  \\\n298  24.00      0      0            19988   30.5000     C106         2   \n884  25.00      0      0  SOTON/OQ 392076    7.0500  B96 B98         2   \n247  24.00      0      2           250649   14.5000  B96 B98         2   \n478  22.00      0      0           350060    7.5208  B96 B98         2   \n305   0.92      1      2           113781  151.5500  C22 C26         2   \n\n      LogFare  \n298  3.449988  \n884  2.085672  \n247  2.740840  \n478  2.142510  \n305  5.027492  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>PassengerId</th>\n      <th>Survived</th>\n      <th>Pclass</th>\n      <th>Name</th>\n      <th>Sex</th>\n      <th>Age</th>\n      <th>SibSp</th>\n      <th>Parch</th>\n      <th>Ticket</th>\n      <th>Fare</th>\n      <th>Cabin</th>\n      <th>Embarked</th>\n      <th>LogFare</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>298</th>\n      <td>299</td>\n      <td>1</td>\n      <td>1</td>\n      <td>Saalfeld, Mr. Adolphe</td>\n      <td>1</td>\n      <td>24.00</td>\n      <td>0</td>\n      <td>0</td>\n      <td>19988</td>\n      <td>30.5000</td>\n      <td>C106</td>\n      <td>2</td>\n      <td>3.449988</td>\n    </tr>\n    <tr>\n      <th>884</th>\n      <td>885</td>\n      <td>0</td>\n      <td>3</td>\n      <td>Sutehall, Mr. Henry Jr</td>\n      <td>1</td>\n      <td>25.00</td>\n      <td>0</td>\n      <td>0</td>\n      <td>SOTON/OQ 392076</td>\n      <td>7.0500</td>\n      <td>B96 B98</td>\n      <td>2</td>\n      <td>2.085672</td>\n    </tr>\n    <tr>\n      <th>247</th>\n      <td>248</td>\n      <td>1</td>\n      <td>2</td>\n      <td>Hamalainen, Mrs. William (Anna)</td>\n      <td>0</td>\n      <td>24.00</td>\n      <td>0</td>\n      <td>2</td>\n      <td>250649</td>\n      <td>14.5000</td>\n      <td>B96 B98</td>\n      <td>2</td>\n      <td>2.740840</td>\n    </tr>\n    <tr>\n      <th>478</th>\n      <td>479</td>\n      <td>0</td>\n      <td>3</td>\n      <td>Karlsson, Mr. Nils August</td>\n      <td>1</td>\n      <td>22.00</td>\n      <td>0</td>\n      <td>0</td>\n      <td>350060</td>\n      <td>7.5208</td>\n      <td>B96 B98</td>\n      <td>2</td>\n      <td>2.142510</td>\n    </tr>\n    <tr>\n      <th>305</th>\n      <td>306</td>\n      <td>1</td>\n      <td>1</td>\n      <td>Allison, Master. Hudson Trevor</td>\n      <td>1</td>\n      <td>0.92</td>\n      <td>1</td>\n      <td>2</td>\n      <td>113781</td>\n      <td>151.5500</td>\n      <td>C22 C26</td>\n      <td>2</td>\n      <td>5.027492</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"(In the previous step we also replaced the categorical variables with their integer codes, since some of the models we'll be building in a moment require that.)\n\nNow we can create our independent variables (the `x` variables) and dependent (the `y` variable):","metadata":{}},{"cell_type":"code","source":"def xs_y(df):\n    xs = df[cats+conts].copy()\n    return xs,df[dep] if dep in df else None\n\ntrn_xs,trn_y = xs_y(trn_df)\nval_xs,val_y = xs_y(val_df)","metadata":{"execution":{"iopub.status.busy":"2022-11-01T10:39:28.170548Z","iopub.execute_input":"2022-11-01T10:39:28.171396Z","iopub.status.idle":"2022-11-01T10:39:28.179378Z","shell.execute_reply.started":"2022-11-01T10:39:28.171354Z","shell.execute_reply":"2022-11-01T10:39:28.178396Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"trn_xs.head()","metadata":{"execution":{"iopub.status.busy":"2022-11-01T10:39:55.630767Z","iopub.execute_input":"2022-11-01T10:39:55.631153Z","iopub.status.idle":"2022-11-01T10:39:55.656454Z","shell.execute_reply.started":"2022-11-01T10:39:55.631105Z","shell.execute_reply":"2022-11-01T10:39:55.655745Z"},"trusted":true},"execution_count":22,"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"     Sex  Embarked    Age  SibSp  Parch   LogFare  Pclass\n298    1         2  24.00      0      0  3.449988       1\n884    1         2  25.00      0      0  2.085672       3\n247    0         2  24.00      0      2  2.740840       2\n478    1         2  22.00      0      0  2.142510       3\n305    1         2   0.92      1      2  5.027492       1","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Sex</th>\n      <th>Embarked</th>\n      <th>Age</th>\n      <th>SibSp</th>\n      <th>Parch</th>\n      <th>LogFare</th>\n      <th>Pclass</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>298</th>\n      <td>1</td>\n      <td>2</td>\n      <td>24.00</td>\n      <td>0</td>\n      <td>0</td>\n      <td>3.449988</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>884</th>\n      <td>1</td>\n      <td>2</td>\n      <td>25.00</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2.085672</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>247</th>\n      <td>0</td>\n      <td>2</td>\n      <td>24.00</td>\n      <td>0</td>\n      <td>2</td>\n      <td>2.740840</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>478</th>\n      <td>1</td>\n      <td>2</td>\n      <td>22.00</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2.142510</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>305</th>\n      <td>1</td>\n      <td>2</td>\n      <td>0.92</td>\n      <td>1</td>\n      <td>2</td>\n      <td>5.027492</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"Here's the predictions for our extremely simple model, where `female` is coded as `0`:","metadata":{}},{"cell_type":"code","source":"preds = val_xs.Sex==0","metadata":{"execution":{"iopub.status.busy":"2022-11-01T10:40:02.356131Z","iopub.execute_input":"2022-11-01T10:40:02.356401Z","iopub.status.idle":"2022-11-01T10:40:02.360589Z","shell.execute_reply.started":"2022-11-01T10:40:02.356369Z","shell.execute_reply":"2022-11-01T10:40:02.359779Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"preds.head()","metadata":{"execution":{"iopub.status.busy":"2022-11-01T10:40:15.523002Z","iopub.execute_input":"2022-11-01T10:40:15.523321Z","iopub.status.idle":"2022-11-01T10:40:15.534095Z","shell.execute_reply.started":"2022-11-01T10:40:15.523280Z","shell.execute_reply":"2022-11-01T10:40:15.533329Z"},"trusted":true},"execution_count":25,"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"709    False\n439    False\n840    False\n720     True\n39      True\nName: Sex, dtype: bool"},"metadata":{}}]},{"cell_type":"markdown","source":"We'll use mean absolute error to measure how good this model is:","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import mean_absolute_error\nmean_absolute_error(val_y, preds)","metadata":{"execution":{"iopub.status.busy":"2022-11-01T10:40:21.545087Z","iopub.execute_input":"2022-11-01T10:40:21.545362Z","iopub.status.idle":"2022-11-01T10:40:21.552771Z","shell.execute_reply.started":"2022-11-01T10:40:21.545330Z","shell.execute_reply":"2022-11-01T10:40:21.551771Z"},"trusted":true},"execution_count":26,"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"0.21524663677130046"},"metadata":{}}]},{"cell_type":"markdown","source":"Alternatively, we could try splitting on a continuous column. We have to use a somewhat different chart to see how this might work -- here's an example of how we could look at `LogFare`:","metadata":{}},{"cell_type":"code","source":"df_fare = trn_df[trn_df.LogFare>0]\nfig,axs = plt.subplots(1,2, figsize=(11,5))\n\n# Boxplot means should be roughly similar in height\nsns.boxenplot(data=df_fare, x=dep, y=\"LogFare\", ax=axs[0])\nsns.kdeplot(data=df_fare, x=\"LogFare\", ax=axs[1]);","metadata":{"execution":{"iopub.status.busy":"2022-11-01T10:40:32.522680Z","iopub.execute_input":"2022-11-01T10:40:32.523016Z","iopub.status.idle":"2022-11-01T10:40:32.970907Z","shell.execute_reply.started":"2022-11-01T10:40:32.522979Z","shell.execute_reply":"2022-11-01T10:40:32.970200Z"},"trusted":true},"execution_count":27,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 792x360 with 2 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"markdown","source":"The [boxenplot](https://seaborn.pydata.org/generated/seaborn.boxenplot.html) above shows quantiles of `LogFare` for each group of `Survived==0` and `Survived==1`. It shows that the average `LogFare` for passengers that didn't survive is around `2.5`, and for those that did it's around `3.2`. So it seems that people that paid more for their tickets were more likely to get put on a lifeboat.\n\nLet's create a simple model based on this observation:","metadata":{}},{"cell_type":"code","source":"preds = val_xs.LogFare>2.7","metadata":{"execution":{"iopub.status.busy":"2022-11-01T10:41:14.886572Z","iopub.execute_input":"2022-11-01T10:41:14.886868Z","iopub.status.idle":"2022-11-01T10:41:14.892063Z","shell.execute_reply.started":"2022-11-01T10:41:14.886833Z","shell.execute_reply":"2022-11-01T10:41:14.890976Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"preds.head()","metadata":{"execution":{"iopub.status.busy":"2022-11-01T10:41:22.887090Z","iopub.execute_input":"2022-11-01T10:41:22.887572Z","iopub.status.idle":"2022-11-01T10:41:22.901004Z","shell.execute_reply.started":"2022-11-01T10:41:22.887497Z","shell.execute_reply":"2022-11-01T10:41:22.900049Z"},"trusted":true},"execution_count":29,"outputs":[{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"709     True\n439    False\n840    False\n720     True\n39     False\nName: LogFare, dtype: bool"},"metadata":{}}]},{"cell_type":"markdown","source":"...and test it out:","metadata":{}},{"cell_type":"code","source":"mean_absolute_error(val_y, preds)","metadata":{"execution":{"iopub.status.busy":"2022-11-01T10:41:30.451070Z","iopub.execute_input":"2022-11-01T10:41:30.451346Z","iopub.status.idle":"2022-11-01T10:41:30.457483Z","shell.execute_reply.started":"2022-11-01T10:41:30.451312Z","shell.execute_reply":"2022-11-01T10:41:30.456649Z"},"trusted":true},"execution_count":30,"outputs":[{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"0.336322869955157"},"metadata":{}}]},{"cell_type":"markdown","source":"This is quite a bit less accurate than our model that used `Sex` as the single binary split.\n\nIdeally, we'd like some way to try more columns and breakpoints more easily. We could create a function that returns how good our model is, in order to more quickly try out a few different splits. We'll create a `score` function to do this. Instead of returning the mean absolute error, we'll calculate a measure of *impurity* -- that is, how much the binary split creates two groups where the rows in a group are each similar to each other, or dissimilar.\n\nWe can measure the similarity of rows inside a group by taking the standard deviation of the dependent variable. If it's higher, then it means the rows are more different to each other. We'll then multiply this by the number of rows, since a bigger group as more impact than a smaller group:","metadata":{}},{"cell_type":"code","source":"def _side_score(side, y):\n    tot = side.sum()\n    if tot<=1: return 0\n    return y[side].std()*tot","metadata":{"execution":{"iopub.status.busy":"2022-11-01T10:41:42.805771Z","iopub.execute_input":"2022-11-01T10:41:42.806049Z","iopub.status.idle":"2022-11-01T10:41:42.810488Z","shell.execute_reply.started":"2022-11-01T10:41:42.806006Z","shell.execute_reply":"2022-11-01T10:41:42.809640Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"markdown","source":"Now we've got that written, we can calculate the score for a split by adding up the scores for the \"left hand side\" (lhs) and \"right hand side\" (rhs):","metadata":{}},{"cell_type":"code","source":"    \ndef score(col, y, split):\n    lhs = col<=split\n    return (_side_score(lhs,y) + _side_score(~lhs,y))/len(y)","metadata":{"execution":{"iopub.status.busy":"2022-11-01T10:42:04.928110Z","iopub.execute_input":"2022-11-01T10:42:04.928456Z","iopub.status.idle":"2022-11-01T10:42:04.935534Z","shell.execute_reply.started":"2022-11-01T10:42:04.928416Z","shell.execute_reply":"2022-11-01T10:42:04.934550Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"markdown","source":"For instance, here's the impurity score for the split on `Sex`:","metadata":{}},{"cell_type":"code","source":"score(trn_xs[\"Sex\"], trn_y, 0.5)","metadata":{"execution":{"iopub.status.busy":"2022-11-01T10:42:16.437484Z","iopub.execute_input":"2022-11-01T10:42:16.438076Z","iopub.status.idle":"2022-11-01T10:42:16.446283Z","shell.execute_reply.started":"2022-11-01T10:42:16.438035Z","shell.execute_reply":"2022-11-01T10:42:16.445562Z"},"trusted":true},"execution_count":34,"outputs":[{"execution_count":34,"output_type":"execute_result","data":{"text/plain":"0.40787530982063946"},"metadata":{}}]},{"cell_type":"markdown","source":"...and for `LogFare`:","metadata":{}},{"cell_type":"code","source":"score(trn_xs[\"LogFare\"], trn_y, 2.7)","metadata":{"execution":{"iopub.status.busy":"2022-11-01T10:42:49.244283Z","iopub.execute_input":"2022-11-01T10:42:49.244765Z","iopub.status.idle":"2022-11-01T10:42:49.252958Z","shell.execute_reply.started":"2022-11-01T10:42:49.244726Z","shell.execute_reply":"2022-11-01T10:42:49.252203Z"},"trusted":true},"execution_count":37,"outputs":[{"execution_count":37,"output_type":"execute_result","data":{"text/plain":"0.47180873952099694"},"metadata":{}}]},{"cell_type":"markdown","source":"As we'd expect from our earlier tests, `Sex` appears to be a better split.\n\nTo make it easier to find the best binary split, we can create a simple interactive tool (note that this only works in Kaggle if you click \"Copy and Edit\" in the top right to open the notebook editor):","metadata":{}},{"cell_type":"code","source":"def iscore(nm, split):\n    col = trn_xs[nm]\n    return score(col, trn_y, split)\n\nfrom ipywidgets import interact\ninteract(nm=conts, split=15.5)(iscore);","metadata":{"execution":{"iopub.status.busy":"2022-11-01T10:42:56.923027Z","iopub.execute_input":"2022-11-01T10:42:56.923334Z","iopub.status.idle":"2022-11-01T10:42:56.967432Z","shell.execute_reply.started":"2022-11-01T10:42:56.923301Z","shell.execute_reply":"2022-11-01T10:42:56.966719Z"},"trusted":true},"execution_count":38,"outputs":[{"output_type":"display_data","data":{"text/plain":"interactive(children=(Dropdown(description='nm', options=('Age', 'SibSp', 'Parch', 'LogFare', 'Pclass'), value…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"369bfc7344724109ad9dc3305524830e"}},"metadata":{}}]},{"cell_type":"markdown","source":"Try selecting different columns and split points using the dropdown and slider above. What splits can you find that increase the purity of the data?\n\nWe can do the same thing for the categorical variables:","metadata":{}},{"cell_type":"code","source":"interact(nm=cats, split=2)(iscore);","metadata":{"execution":{"iopub.status.busy":"2022-11-01T10:44:07.313283Z","iopub.execute_input":"2022-11-01T10:44:07.313587Z","iopub.status.idle":"2022-11-01T10:44:07.352143Z","shell.execute_reply.started":"2022-11-01T10:44:07.313543Z","shell.execute_reply":"2022-11-01T10:44:07.351376Z"},"trusted":true},"execution_count":40,"outputs":[{"output_type":"display_data","data":{"text/plain":"interactive(children=(Dropdown(description='nm', options=('Sex', 'Embarked'), value='Sex'), IntSlider(value=2,…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"84269950321a4198b6ab6242395c376c"}},"metadata":{}}]},{"cell_type":"markdown","source":"That works well enough, but it's rather slow and fiddly. Perhaps we could get the computer to automatically find the best split point for a column for us? For example, to find the best split point for `age` we'd first need to make a list of all the possible split points (i.e all the unique values of that field)...:","metadata":{}},{"cell_type":"code","source":"nm = \"Age\"\ncol = trn_xs[nm]\nunq = col.unique()\nunq.sort()\nunq","metadata":{"execution":{"iopub.status.busy":"2022-11-01T10:44:14.943362Z","iopub.execute_input":"2022-11-01T10:44:14.943956Z","iopub.status.idle":"2022-11-01T10:44:14.952430Z","shell.execute_reply.started":"2022-11-01T10:44:14.943916Z","shell.execute_reply":"2022-11-01T10:44:14.951710Z"},"trusted":true},"execution_count":42,"outputs":[{"execution_count":42,"output_type":"execute_result","data":{"text/plain":"array([ 0.42,  0.67,  0.75,  0.83,  0.92,  1.  ,  2.  ,  3.  ,  4.  ,  5.  ,  6.  ,  7.  ,  8.  ,  9.  , 10.  , 11.  , 12.  ,\n       13.  , 14.  , 14.5 , 15.  , 16.  , 17.  , 18.  , 19.  , 20.  , 21.  , 22.  , 23.  , 24.  , 24.5 , 25.  , 26.  , 27.  ,\n       28.  , 28.5 , 29.  , 30.  , 31.  , 32.  , 32.5 , 33.  , 34.  , 34.5 , 35.  , 36.  , 36.5 , 37.  , 38.  , 39.  , 40.  ,\n       40.5 , 41.  , 42.  , 43.  , 44.  , 45.  , 45.5 , 46.  , 47.  , 48.  , 49.  , 50.  , 51.  , 52.  , 53.  , 54.  , 55.  ,\n       55.5 , 56.  , 57.  , 58.  , 59.  , 60.  , 61.  , 62.  , 64.  , 65.  , 70.  , 70.5 , 74.  , 80.  ])"},"metadata":{}}]},{"cell_type":"markdown","source":"...and find which index of those values is where `score()` is the lowest:","metadata":{}},{"cell_type":"code","source":"scores = np.array([score(col, trn_y, o) for o in unq if not np.isnan(o)])\nunq[scores.argmin()]","metadata":{"execution":{"iopub.status.busy":"2022-11-01T10:45:01.875415Z","iopub.execute_input":"2022-11-01T10:45:01.876195Z","iopub.status.idle":"2022-11-01T10:45:01.957668Z","shell.execute_reply.started":"2022-11-01T10:45:01.876158Z","shell.execute_reply":"2022-11-01T10:45:01.956715Z"},"trusted":true},"execution_count":45,"outputs":[{"execution_count":45,"output_type":"execute_result","data":{"text/plain":"6.0"},"metadata":{}}]},{"cell_type":"markdown","source":"Based on this, it looks like, for instance, that for the `Age` column, `6` is the optimal cutoff according to our training set.\n\nWe can write a little function that implements this idea:","metadata":{}},{"cell_type":"code","source":"def min_col(df, nm):\n    col,y = df[nm],df[dep]\n    unq = col.dropna().unique()\n    scores = np.array([score(col, y, o) for o in unq if not np.isnan(o)])\n    idx = scores.argmin()\n    return unq[idx],scores[idx]\n\nmin_col(trn_df, \"Age\")","metadata":{"execution":{"iopub.status.busy":"2022-11-01T10:45:03.859170Z","iopub.execute_input":"2022-11-01T10:45:03.859446Z","iopub.status.idle":"2022-11-01T10:45:03.948216Z","shell.execute_reply.started":"2022-11-01T10:45:03.859417Z","shell.execute_reply":"2022-11-01T10:45:03.947353Z"},"trusted":true},"execution_count":46,"outputs":[{"execution_count":46,"output_type":"execute_result","data":{"text/plain":"(6.0, 0.478316717508991)"},"metadata":{}}]},{"cell_type":"markdown","source":"Let's try all the columns:","metadata":{}},{"cell_type":"code","source":"cols = cats+conts\n{o:min_col(trn_df, o) for o in cols}","metadata":{"execution":{"iopub.status.busy":"2022-11-01T10:46:44.431404Z","iopub.execute_input":"2022-11-01T10:46:44.432014Z","iopub.status.idle":"2022-11-01T10:46:44.722624Z","shell.execute_reply.started":"2022-11-01T10:46:44.431975Z","shell.execute_reply":"2022-11-01T10:46:44.721774Z"},"trusted":true},"execution_count":48,"outputs":[{"execution_count":48,"output_type":"execute_result","data":{"text/plain":"{'Sex': (0, 0.40787530982063946),\n 'Embarked': (0, 0.47883342573147836),\n 'Age': (6.0, 0.478316717508991),\n 'SibSp': (4, 0.4783740258817434),\n 'Parch': (0, 0.4805296527841601),\n 'LogFare': (2.4390808375825834, 0.4620823937736597),\n 'Pclass': (2, 0.46048261885806596)}"},"metadata":{}}]},{"cell_type":"markdown","source":"According to this, `Sex<=0` is the best split we can use.\n\nWe've just re-invented the [OneR](https://link.springer.com/article/10.1023/A:1022631118932) classifier (or at least, a minor variant of it), which was found to be one of the most effective classifiers in real-world datasets, compared to the algorithms in use in 1993. Since it's so simple and surprisingly effective, it makes for a great *baseline* -- that is, a starting point that you can use to compare your more sophisticated models to.\n\nWe found earlier that out OneR rule had an error of around `0.215`, so we'll keep that in mind as we try out more sophisticated approaches.","metadata":{}},{"cell_type":"markdown","source":"## Creating a decision tree","metadata":{}},{"cell_type":"markdown","source":"How can we improve our OneR classifier, which predicts survival based only on `Sex`?\n\nHow about we take each of our two groups, `female` and `male`, and create one more binary split for each of them. That is: fine the single best split for females, and the single best split for males. To do this, all we have to do is repeat the previous section's steps, once for males, and once for females.\n\nFirst, we'll remove `Sex` from the list of possible splits (since we've already used it, and there's only one possible split for that binary column), and create our two groups:","metadata":{}},{"cell_type":"code","source":"cols.remove(\"Sex\")\nismale = trn_df.Sex==1\nmales,females = trn_df[ismale],trn_df[~ismale]","metadata":{"execution":{"iopub.status.busy":"2022-11-01T10:46:56.789826Z","iopub.execute_input":"2022-11-01T10:46:56.790112Z","iopub.status.idle":"2022-11-01T10:46:56.795858Z","shell.execute_reply.started":"2022-11-01T10:46:56.790080Z","shell.execute_reply":"2022-11-01T10:46:56.795106Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"code","source":"males.head()","metadata":{"execution":{"iopub.status.busy":"2022-11-01T10:47:10.116709Z","iopub.execute_input":"2022-11-01T10:47:10.117085Z","iopub.status.idle":"2022-11-01T10:47:10.141830Z","shell.execute_reply.started":"2022-11-01T10:47:10.117040Z","shell.execute_reply":"2022-11-01T10:47:10.139381Z"},"trusted":true},"execution_count":51,"outputs":[{"execution_count":51,"output_type":"execute_result","data":{"text/plain":"     PassengerId  Survived  Pclass                            Name  Sex  \\\n298          299         1       1           Saalfeld, Mr. Adolphe    1   \n884          885         0       3          Sutehall, Mr. Henry Jr    1   \n478          479         0       3       Karlsson, Mr. Nils August    1   \n305          306         1       1  Allison, Master. Hudson Trevor    1   \n405          406         0       2              Gale, Mr. Shadrach    1   \n\n       Age  SibSp  Parch           Ticket      Fare    Cabin  Embarked  \\\n298  24.00      0      0            19988   30.5000     C106         2   \n884  25.00      0      0  SOTON/OQ 392076    7.0500  B96 B98         2   \n478  22.00      0      0           350060    7.5208  B96 B98         2   \n305   0.92      1      2           113781  151.5500  C22 C26         2   \n405  34.00      1      0            28664   21.0000  B96 B98         2   \n\n      LogFare  \n298  3.449988  \n884  2.085672  \n478  2.142510  \n305  5.027492  \n405  3.091042  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>PassengerId</th>\n      <th>Survived</th>\n      <th>Pclass</th>\n      <th>Name</th>\n      <th>Sex</th>\n      <th>Age</th>\n      <th>SibSp</th>\n      <th>Parch</th>\n      <th>Ticket</th>\n      <th>Fare</th>\n      <th>Cabin</th>\n      <th>Embarked</th>\n      <th>LogFare</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>298</th>\n      <td>299</td>\n      <td>1</td>\n      <td>1</td>\n      <td>Saalfeld, Mr. Adolphe</td>\n      <td>1</td>\n      <td>24.00</td>\n      <td>0</td>\n      <td>0</td>\n      <td>19988</td>\n      <td>30.5000</td>\n      <td>C106</td>\n      <td>2</td>\n      <td>3.449988</td>\n    </tr>\n    <tr>\n      <th>884</th>\n      <td>885</td>\n      <td>0</td>\n      <td>3</td>\n      <td>Sutehall, Mr. Henry Jr</td>\n      <td>1</td>\n      <td>25.00</td>\n      <td>0</td>\n      <td>0</td>\n      <td>SOTON/OQ 392076</td>\n      <td>7.0500</td>\n      <td>B96 B98</td>\n      <td>2</td>\n      <td>2.085672</td>\n    </tr>\n    <tr>\n      <th>478</th>\n      <td>479</td>\n      <td>0</td>\n      <td>3</td>\n      <td>Karlsson, Mr. Nils August</td>\n      <td>1</td>\n      <td>22.00</td>\n      <td>0</td>\n      <td>0</td>\n      <td>350060</td>\n      <td>7.5208</td>\n      <td>B96 B98</td>\n      <td>2</td>\n      <td>2.142510</td>\n    </tr>\n    <tr>\n      <th>305</th>\n      <td>306</td>\n      <td>1</td>\n      <td>1</td>\n      <td>Allison, Master. Hudson Trevor</td>\n      <td>1</td>\n      <td>0.92</td>\n      <td>1</td>\n      <td>2</td>\n      <td>113781</td>\n      <td>151.5500</td>\n      <td>C22 C26</td>\n      <td>2</td>\n      <td>5.027492</td>\n    </tr>\n    <tr>\n      <th>405</th>\n      <td>406</td>\n      <td>0</td>\n      <td>2</td>\n      <td>Gale, Mr. Shadrach</td>\n      <td>1</td>\n      <td>34.00</td>\n      <td>1</td>\n      <td>0</td>\n      <td>28664</td>\n      <td>21.0000</td>\n      <td>B96 B98</td>\n      <td>2</td>\n      <td>3.091042</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"females.head()","metadata":{"execution":{"iopub.status.busy":"2022-11-01T10:47:21.557727Z","iopub.execute_input":"2022-11-01T10:47:21.558051Z","iopub.status.idle":"2022-11-01T10:47:21.581746Z","shell.execute_reply.started":"2022-11-01T10:47:21.558011Z","shell.execute_reply":"2022-11-01T10:47:21.580809Z"},"trusted":true},"execution_count":52,"outputs":[{"execution_count":52,"output_type":"execute_result","data":{"text/plain":"     PassengerId  Survived  Pclass                                   Name  \\\n247          248         1       2        Hamalainen, Mrs. William (Anna)   \n316          317         1       2    Kantor, Mrs. Sinai (Miriam Sternin)   \n2              3         1       3                 Heikkinen, Miss. Laina   \n742          743         1       1  Ryerson, Miss. Susan Parker \"Suzette\"   \n367          368         1       3         Moussa, Mrs. (Mantoura Boulos)   \n\n     Sex   Age  SibSp  Parch            Ticket      Fare            Cabin  \\\n247    0  24.0      0      2            250649   14.5000          B96 B98   \n316    0  24.0      1      0            244367   26.0000          B96 B98   \n2      0  26.0      0      0  STON/O2. 3101282    7.9250          B96 B98   \n742    0  21.0      2      2          PC 17608  262.3750  B57 B59 B63 B66   \n367    0  24.0      0      0              2626    7.2292          B96 B98   \n\n     Embarked   LogFare  \n247         2  2.740840  \n316         2  3.295837  \n2           2  2.188856  \n742         0  5.573579  \n367         0  2.107689  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>PassengerId</th>\n      <th>Survived</th>\n      <th>Pclass</th>\n      <th>Name</th>\n      <th>Sex</th>\n      <th>Age</th>\n      <th>SibSp</th>\n      <th>Parch</th>\n      <th>Ticket</th>\n      <th>Fare</th>\n      <th>Cabin</th>\n      <th>Embarked</th>\n      <th>LogFare</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>247</th>\n      <td>248</td>\n      <td>1</td>\n      <td>2</td>\n      <td>Hamalainen, Mrs. William (Anna)</td>\n      <td>0</td>\n      <td>24.0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>250649</td>\n      <td>14.5000</td>\n      <td>B96 B98</td>\n      <td>2</td>\n      <td>2.740840</td>\n    </tr>\n    <tr>\n      <th>316</th>\n      <td>317</td>\n      <td>1</td>\n      <td>2</td>\n      <td>Kantor, Mrs. Sinai (Miriam Sternin)</td>\n      <td>0</td>\n      <td>24.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>244367</td>\n      <td>26.0000</td>\n      <td>B96 B98</td>\n      <td>2</td>\n      <td>3.295837</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>1</td>\n      <td>3</td>\n      <td>Heikkinen, Miss. Laina</td>\n      <td>0</td>\n      <td>26.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>STON/O2. 3101282</td>\n      <td>7.9250</td>\n      <td>B96 B98</td>\n      <td>2</td>\n      <td>2.188856</td>\n    </tr>\n    <tr>\n      <th>742</th>\n      <td>743</td>\n      <td>1</td>\n      <td>1</td>\n      <td>Ryerson, Miss. Susan Parker \"Suzette\"</td>\n      <td>0</td>\n      <td>21.0</td>\n      <td>2</td>\n      <td>2</td>\n      <td>PC 17608</td>\n      <td>262.3750</td>\n      <td>B57 B59 B63 B66</td>\n      <td>0</td>\n      <td>5.573579</td>\n    </tr>\n    <tr>\n      <th>367</th>\n      <td>368</td>\n      <td>1</td>\n      <td>3</td>\n      <td>Moussa, Mrs. (Mantoura Boulos)</td>\n      <td>0</td>\n      <td>24.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2626</td>\n      <td>7.2292</td>\n      <td>B96 B98</td>\n      <td>0</td>\n      <td>2.107689</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"Now let's find the single best binary split for males...:","metadata":{}},{"cell_type":"code","source":"{o:min_col(males, o) for o in cols}\n# Seems to be age","metadata":{"execution":{"iopub.status.busy":"2022-11-01T10:47:35.364775Z","iopub.execute_input":"2022-11-01T10:47:35.365053Z","iopub.status.idle":"2022-11-01T10:47:35.609009Z","shell.execute_reply.started":"2022-11-01T10:47:35.365022Z","shell.execute_reply":"2022-11-01T10:47:35.608129Z"},"trusted":true},"execution_count":53,"outputs":[{"execution_count":53,"output_type":"execute_result","data":{"text/plain":"{'Embarked': (0, 0.3875581870410906),\n 'Age': (6.0, 0.3739828371010595),\n 'SibSp': (4, 0.3875864227586273),\n 'Parch': (0, 0.3874704821461959),\n 'LogFare': (2.803360380906535, 0.3804856231758151),\n 'Pclass': (1, 0.38155442004360934)}"},"metadata":{}}]},{"cell_type":"markdown","source":"...and for females:","metadata":{}},{"cell_type":"code","source":"{o:min_col(females, o) for o in cols}\n# Seems to be PClass","metadata":{"execution":{"iopub.status.busy":"2022-11-01T10:48:29.665831Z","iopub.execute_input":"2022-11-01T10:48:29.666130Z","iopub.status.idle":"2022-11-01T10:48:29.877487Z","shell.execute_reply.started":"2022-11-01T10:48:29.666095Z","shell.execute_reply":"2022-11-01T10:48:29.875638Z"},"trusted":true},"execution_count":55,"outputs":[{"execution_count":55,"output_type":"execute_result","data":{"text/plain":"{'Embarked': (0, 0.4295252982857327),\n 'Age': (50.0, 0.4225927658431649),\n 'SibSp': (4, 0.42319212059713535),\n 'Parch': (3, 0.4193314500446158),\n 'LogFare': (4.256321678298823, 0.41350598332911376),\n 'Pclass': (2, 0.3335388911567601)}"},"metadata":{}}]},{"cell_type":"markdown","source":"We can see that the best next binary split for males is `Age<=6`, and for females is `Pclass<=2`.\n\nBy adding these rules, we have created a *decision tree*, where our model will first check whether `Sex` is female or male, and depending on the result will then check either the above `Age` or `Pclass` rules, as appropriate. We could then repeat the process, creating new additional rules for each of the four groups we've now created.\n\nRather than writing that code manually, we can use `DecisionTreeClassifier`, from *sklearn*, which does exactly that for us:","metadata":{}},{"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier, export_graphviz\n\nm = DecisionTreeClassifier(max_leaf_nodes=4).fit(trn_xs, trn_y);","metadata":{"execution":{"iopub.status.busy":"2022-11-01T10:48:38.602223Z","iopub.execute_input":"2022-11-01T10:48:38.602503Z","iopub.status.idle":"2022-11-01T10:48:38.786473Z","shell.execute_reply.started":"2022-11-01T10:48:38.602457Z","shell.execute_reply":"2022-11-01T10:48:38.785594Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"code","source":"help(m)","metadata":{"execution":{"iopub.status.busy":"2022-11-01T10:48:51.823956Z","iopub.execute_input":"2022-11-01T10:48:51.824241Z","iopub.status.idle":"2022-11-01T10:48:51.836870Z","shell.execute_reply.started":"2022-11-01T10:48:51.824208Z","shell.execute_reply":"2022-11-01T10:48:51.835858Z"},"trusted":true},"execution_count":58,"outputs":[{"name":"stdout","text":"Help on DecisionTreeClassifier in module sklearn.tree._classes object:\n\nclass DecisionTreeClassifier(sklearn.base.ClassifierMixin, BaseDecisionTree)\n |  DecisionTreeClassifier(*, criterion='gini', splitter='best', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=None, random_state=None, max_leaf_nodes=None, min_impurity_decrease=0.0, class_weight=None, ccp_alpha=0.0)\n |  \n |  A decision tree classifier.\n |  \n |  Read more in the :ref:`User Guide <tree>`.\n |  \n |  Parameters\n |  ----------\n |  criterion : {\"gini\", \"entropy\"}, default=\"gini\"\n |      The function to measure the quality of a split. Supported criteria are\n |      \"gini\" for the Gini impurity and \"entropy\" for the information gain.\n |  \n |  splitter : {\"best\", \"random\"}, default=\"best\"\n |      The strategy used to choose the split at each node. Supported\n |      strategies are \"best\" to choose the best split and \"random\" to choose\n |      the best random split.\n |  \n |  max_depth : int, default=None\n |      The maximum depth of the tree. If None, then nodes are expanded until\n |      all leaves are pure or until all leaves contain less than\n |      min_samples_split samples.\n |  \n |  min_samples_split : int or float, default=2\n |      The minimum number of samples required to split an internal node:\n |  \n |      - If int, then consider `min_samples_split` as the minimum number.\n |      - If float, then `min_samples_split` is a fraction and\n |        `ceil(min_samples_split * n_samples)` are the minimum\n |        number of samples for each split.\n |  \n |      .. versionchanged:: 0.18\n |         Added float values for fractions.\n |  \n |  min_samples_leaf : int or float, default=1\n |      The minimum number of samples required to be at a leaf node.\n |      A split point at any depth will only be considered if it leaves at\n |      least ``min_samples_leaf`` training samples in each of the left and\n |      right branches.  This may have the effect of smoothing the model,\n |      especially in regression.\n |  \n |      - If int, then consider `min_samples_leaf` as the minimum number.\n |      - If float, then `min_samples_leaf` is a fraction and\n |        `ceil(min_samples_leaf * n_samples)` are the minimum\n |        number of samples for each node.\n |  \n |      .. versionchanged:: 0.18\n |         Added float values for fractions.\n |  \n |  min_weight_fraction_leaf : float, default=0.0\n |      The minimum weighted fraction of the sum total of weights (of all\n |      the input samples) required to be at a leaf node. Samples have\n |      equal weight when sample_weight is not provided.\n |  \n |  max_features : int, float or {\"auto\", \"sqrt\", \"log2\"}, default=None\n |      The number of features to consider when looking for the best split:\n |  \n |          - If int, then consider `max_features` features at each split.\n |          - If float, then `max_features` is a fraction and\n |            `int(max_features * n_features)` features are considered at each\n |            split.\n |          - If \"auto\", then `max_features=sqrt(n_features)`.\n |          - If \"sqrt\", then `max_features=sqrt(n_features)`.\n |          - If \"log2\", then `max_features=log2(n_features)`.\n |          - If None, then `max_features=n_features`.\n |  \n |      Note: the search for a split does not stop until at least one\n |      valid partition of the node samples is found, even if it requires to\n |      effectively inspect more than ``max_features`` features.\n |  \n |  random_state : int, RandomState instance or None, default=None\n |      Controls the randomness of the estimator. The features are always\n |      randomly permuted at each split, even if ``splitter`` is set to\n |      ``\"best\"``. When ``max_features < n_features``, the algorithm will\n |      select ``max_features`` at random at each split before finding the best\n |      split among them. But the best found split may vary across different\n |      runs, even if ``max_features=n_features``. That is the case, if the\n |      improvement of the criterion is identical for several splits and one\n |      split has to be selected at random. To obtain a deterministic behaviour\n |      during fitting, ``random_state`` has to be fixed to an integer.\n |      See :term:`Glossary <random_state>` for details.\n |  \n |  max_leaf_nodes : int, default=None\n |      Grow a tree with ``max_leaf_nodes`` in best-first fashion.\n |      Best nodes are defined as relative reduction in impurity.\n |      If None then unlimited number of leaf nodes.\n |  \n |  min_impurity_decrease : float, default=0.0\n |      A node will be split if this split induces a decrease of the impurity\n |      greater than or equal to this value.\n |  \n |      The weighted impurity decrease equation is the following::\n |  \n |          N_t / N * (impurity - N_t_R / N_t * right_impurity\n |                              - N_t_L / N_t * left_impurity)\n |  \n |      where ``N`` is the total number of samples, ``N_t`` is the number of\n |      samples at the current node, ``N_t_L`` is the number of samples in the\n |      left child, and ``N_t_R`` is the number of samples in the right child.\n |  \n |      ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n |      if ``sample_weight`` is passed.\n |  \n |      .. versionadded:: 0.19\n |  \n |  class_weight : dict, list of dict or \"balanced\", default=None\n |      Weights associated with classes in the form ``{class_label: weight}``.\n |      If None, all classes are supposed to have weight one. For\n |      multi-output problems, a list of dicts can be provided in the same\n |      order as the columns of y.\n |  \n |      Note that for multioutput (including multilabel) weights should be\n |      defined for each class of every column in its own dict. For example,\n |      for four-class multilabel classification weights should be\n |      [{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of\n |      [{1:1}, {2:5}, {3:1}, {4:1}].\n |  \n |      The \"balanced\" mode uses the values of y to automatically adjust\n |      weights inversely proportional to class frequencies in the input data\n |      as ``n_samples / (n_classes * np.bincount(y))``\n |  \n |      For multi-output, the weights of each column of y will be multiplied.\n |  \n |      Note that these weights will be multiplied with sample_weight (passed\n |      through the fit method) if sample_weight is specified.\n |  \n |  ccp_alpha : non-negative float, default=0.0\n |      Complexity parameter used for Minimal Cost-Complexity Pruning. The\n |      subtree with the largest cost complexity that is smaller than\n |      ``ccp_alpha`` will be chosen. By default, no pruning is performed. See\n |      :ref:`minimal_cost_complexity_pruning` for details.\n |  \n |      .. versionadded:: 0.22\n |  \n |  Attributes\n |  ----------\n |  classes_ : ndarray of shape (n_classes,) or list of ndarray\n |      The classes labels (single output problem),\n |      or a list of arrays of class labels (multi-output problem).\n |  \n |  feature_importances_ : ndarray of shape (n_features,)\n |      The impurity-based feature importances.\n |      The higher, the more important the feature.\n |      The importance of a feature is computed as the (normalized)\n |      total reduction of the criterion brought by that feature.  It is also\n |      known as the Gini importance [4]_.\n |  \n |      Warning: impurity-based feature importances can be misleading for\n |      high cardinality features (many unique values). See\n |      :func:`sklearn.inspection.permutation_importance` as an alternative.\n |  \n |  max_features_ : int\n |      The inferred value of max_features.\n |  \n |  n_classes_ : int or list of int\n |      The number of classes (for single output problems),\n |      or a list containing the number of classes for each\n |      output (for multi-output problems).\n |  \n |  n_features_ : int\n |      The number of features when ``fit`` is performed.\n |  \n |      .. deprecated:: 1.0\n |         `n_features_` is deprecated in 1.0 and will be removed in\n |         1.2. Use `n_features_in_` instead.\n |  \n |  n_features_in_ : int\n |      Number of features seen during :term:`fit`.\n |  \n |      .. versionadded:: 0.24\n |  \n |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n |      Names of features seen during :term:`fit`. Defined only when `X`\n |      has feature names that are all strings.\n |  \n |      .. versionadded:: 1.0\n |  \n |  n_outputs_ : int\n |      The number of outputs when ``fit`` is performed.\n |  \n |  tree_ : Tree instance\n |      The underlying Tree object. Please refer to\n |      ``help(sklearn.tree._tree.Tree)`` for attributes of Tree object and\n |      :ref:`sphx_glr_auto_examples_tree_plot_unveil_tree_structure.py`\n |      for basic usage of these attributes.\n |  \n |  See Also\n |  --------\n |  DecisionTreeRegressor : A decision tree regressor.\n |  \n |  Notes\n |  -----\n |  The default values for the parameters controlling the size of the trees\n |  (e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\n |  unpruned trees which can potentially be very large on some data sets. To\n |  reduce memory consumption, the complexity and size of the trees should be\n |  controlled by setting those parameter values.\n |  \n |  The :meth:`predict` method operates using the :func:`numpy.argmax`\n |  function on the outputs of :meth:`predict_proba`. This means that in\n |  case the highest predicted probabilities are tied, the classifier will\n |  predict the tied class with the lowest index in :term:`classes_`.\n |  \n |  References\n |  ----------\n |  \n |  .. [1] https://en.wikipedia.org/wiki/Decision_tree_learning\n |  \n |  .. [2] L. Breiman, J. Friedman, R. Olshen, and C. Stone, \"Classification\n |         and Regression Trees\", Wadsworth, Belmont, CA, 1984.\n |  \n |  .. [3] T. Hastie, R. Tibshirani and J. Friedman. \"Elements of Statistical\n |         Learning\", Springer, 2009.\n |  \n |  .. [4] L. Breiman, and A. Cutler, \"Random Forests\",\n |         https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm\n |  \n |  Examples\n |  --------\n |  >>> from sklearn.datasets import load_iris\n |  >>> from sklearn.model_selection import cross_val_score\n |  >>> from sklearn.tree import DecisionTreeClassifier\n |  >>> clf = DecisionTreeClassifier(random_state=0)\n |  >>> iris = load_iris()\n |  >>> cross_val_score(clf, iris.data, iris.target, cv=10)\n |  ...                             # doctest: +SKIP\n |  ...\n |  array([ 1.     ,  0.93...,  0.86...,  0.93...,  0.93...,\n |          0.93...,  0.93...,  1.     ,  0.93...,  1.      ])\n |  \n |  Method resolution order:\n |      DecisionTreeClassifier\n |      sklearn.base.ClassifierMixin\n |      BaseDecisionTree\n |      sklearn.base.MultiOutputMixin\n |      sklearn.base.BaseEstimator\n |      builtins.object\n |  \n |  Methods defined here:\n |  \n |  __init__(self, *, criterion='gini', splitter='best', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=None, random_state=None, max_leaf_nodes=None, min_impurity_decrease=0.0, class_weight=None, ccp_alpha=0.0)\n |      Initialize self.  See help(type(self)) for accurate signature.\n |  \n |  fit(self, X, y, sample_weight=None, check_input=True, X_idx_sorted='deprecated')\n |      Build a decision tree classifier from the training set (X, y).\n |      \n |      Parameters\n |      ----------\n |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n |          The training input samples. Internally, it will be converted to\n |          ``dtype=np.float32`` and if a sparse matrix is provided\n |          to a sparse ``csc_matrix``.\n |      \n |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n |          The target values (class labels) as integers or strings.\n |      \n |      sample_weight : array-like of shape (n_samples,), default=None\n |          Sample weights. If None, then samples are equally weighted. Splits\n |          that would create child nodes with net zero or negative weight are\n |          ignored while searching for a split in each node. Splits are also\n |          ignored if they would result in any single class carrying a\n |          negative weight in either child node.\n |      \n |      check_input : bool, default=True\n |          Allow to bypass several input checking.\n |          Don't use this parameter unless you know what you do.\n |      \n |      X_idx_sorted : deprecated, default=\"deprecated\"\n |          This parameter is deprecated and has no effect.\n |          It will be removed in 1.1 (renaming of 0.26).\n |      \n |          .. deprecated:: 0.24\n |      \n |      Returns\n |      -------\n |      self : DecisionTreeClassifier\n |          Fitted estimator.\n |  \n |  predict_log_proba(self, X)\n |      Predict class log-probabilities of the input samples X.\n |      \n |      Parameters\n |      ----------\n |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n |          The input samples. Internally, it will be converted to\n |          ``dtype=np.float32`` and if a sparse matrix is provided\n |          to a sparse ``csr_matrix``.\n |      \n |      Returns\n |      -------\n |      proba : ndarray of shape (n_samples, n_classes) or list of n_outputs             such arrays if n_outputs > 1\n |          The class log-probabilities of the input samples. The order of the\n |          classes corresponds to that in the attribute :term:`classes_`.\n |  \n |  predict_proba(self, X, check_input=True)\n |      Predict class probabilities of the input samples X.\n |      \n |      The predicted class probability is the fraction of samples of the same\n |      class in a leaf.\n |      \n |      Parameters\n |      ----------\n |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n |          The input samples. Internally, it will be converted to\n |          ``dtype=np.float32`` and if a sparse matrix is provided\n |          to a sparse ``csr_matrix``.\n |      \n |      check_input : bool, default=True\n |          Allow to bypass several input checking.\n |          Don't use this parameter unless you know what you do.\n |      \n |      Returns\n |      -------\n |      proba : ndarray of shape (n_samples, n_classes) or list of n_outputs             such arrays if n_outputs > 1\n |          The class probabilities of the input samples. The order of the\n |          classes corresponds to that in the attribute :term:`classes_`.\n |  \n |  ----------------------------------------------------------------------\n |  Data descriptors defined here:\n |  \n |  n_features_\n |      DEPRECATED: The attribute `n_features_` is deprecated in 1.0 and will be removed in 1.2. Use `n_features_in_` instead.\n |  \n |  ----------------------------------------------------------------------\n |  Data and other attributes defined here:\n |  \n |  __abstractmethods__ = frozenset()\n |  \n |  ----------------------------------------------------------------------\n |  Methods inherited from sklearn.base.ClassifierMixin:\n |  \n |  score(self, X, y, sample_weight=None)\n |      Return the mean accuracy on the given test data and labels.\n |      \n |      In multi-label classification, this is the subset accuracy\n |      which is a harsh metric since you require for each sample that\n |      each label set be correctly predicted.\n |      \n |      Parameters\n |      ----------\n |      X : array-like of shape (n_samples, n_features)\n |          Test samples.\n |      \n |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n |          True labels for `X`.\n |      \n |      sample_weight : array-like of shape (n_samples,), default=None\n |          Sample weights.\n |      \n |      Returns\n |      -------\n |      score : float\n |          Mean accuracy of ``self.predict(X)`` wrt. `y`.\n |  \n |  ----------------------------------------------------------------------\n |  Data descriptors inherited from sklearn.base.ClassifierMixin:\n |  \n |  __dict__\n |      dictionary for instance variables (if defined)\n |  \n |  __weakref__\n |      list of weak references to the object (if defined)\n |  \n |  ----------------------------------------------------------------------\n |  Methods inherited from BaseDecisionTree:\n |  \n |  apply(self, X, check_input=True)\n |      Return the index of the leaf that each sample is predicted as.\n |      \n |      .. versionadded:: 0.17\n |      \n |      Parameters\n |      ----------\n |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n |          The input samples. Internally, it will be converted to\n |          ``dtype=np.float32`` and if a sparse matrix is provided\n |          to a sparse ``csr_matrix``.\n |      \n |      check_input : bool, default=True\n |          Allow to bypass several input checking.\n |          Don't use this parameter unless you know what you do.\n |      \n |      Returns\n |      -------\n |      X_leaves : array-like of shape (n_samples,)\n |          For each datapoint x in X, return the index of the leaf x\n |          ends up in. Leaves are numbered within\n |          ``[0; self.tree_.node_count)``, possibly with gaps in the\n |          numbering.\n |  \n |  cost_complexity_pruning_path(self, X, y, sample_weight=None)\n |      Compute the pruning path during Minimal Cost-Complexity Pruning.\n |      \n |      See :ref:`minimal_cost_complexity_pruning` for details on the pruning\n |      process.\n |      \n |      Parameters\n |      ----------\n |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n |          The training input samples. Internally, it will be converted to\n |          ``dtype=np.float32`` and if a sparse matrix is provided\n |          to a sparse ``csc_matrix``.\n |      \n |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n |          The target values (class labels) as integers or strings.\n |      \n |      sample_weight : array-like of shape (n_samples,), default=None\n |          Sample weights. If None, then samples are equally weighted. Splits\n |          that would create child nodes with net zero or negative weight are\n |          ignored while searching for a split in each node. Splits are also\n |          ignored if they would result in any single class carrying a\n |          negative weight in either child node.\n |      \n |      Returns\n |      -------\n |      ccp_path : :class:`~sklearn.utils.Bunch`\n |          Dictionary-like object, with the following attributes.\n |      \n |          ccp_alphas : ndarray\n |              Effective alphas of subtree during pruning.\n |      \n |          impurities : ndarray\n |              Sum of the impurities of the subtree leaves for the\n |              corresponding alpha value in ``ccp_alphas``.\n |  \n |  decision_path(self, X, check_input=True)\n |      Return the decision path in the tree.\n |      \n |      .. versionadded:: 0.18\n |      \n |      Parameters\n |      ----------\n |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n |          The input samples. Internally, it will be converted to\n |          ``dtype=np.float32`` and if a sparse matrix is provided\n |          to a sparse ``csr_matrix``.\n |      \n |      check_input : bool, default=True\n |          Allow to bypass several input checking.\n |          Don't use this parameter unless you know what you do.\n |      \n |      Returns\n |      -------\n |      indicator : sparse matrix of shape (n_samples, n_nodes)\n |          Return a node indicator CSR matrix where non zero elements\n |          indicates that the samples goes through the nodes.\n |  \n |  get_depth(self)\n |      Return the depth of the decision tree.\n |      \n |      The depth of a tree is the maximum distance between the root\n |      and any leaf.\n |      \n |      Returns\n |      -------\n |      self.tree_.max_depth : int\n |          The maximum depth of the tree.\n |  \n |  get_n_leaves(self)\n |      Return the number of leaves of the decision tree.\n |      \n |      Returns\n |      -------\n |      self.tree_.n_leaves : int\n |          Number of leaves.\n |  \n |  predict(self, X, check_input=True)\n |      Predict class or regression value for X.\n |      \n |      For a classification model, the predicted class for each sample in X is\n |      returned. For a regression model, the predicted value based on X is\n |      returned.\n |      \n |      Parameters\n |      ----------\n |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n |          The input samples. Internally, it will be converted to\n |          ``dtype=np.float32`` and if a sparse matrix is provided\n |          to a sparse ``csr_matrix``.\n |      \n |      check_input : bool, default=True\n |          Allow to bypass several input checking.\n |          Don't use this parameter unless you know what you do.\n |      \n |      Returns\n |      -------\n |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n |          The predicted classes, or the predict values.\n |  \n |  ----------------------------------------------------------------------\n |  Data descriptors inherited from BaseDecisionTree:\n |  \n |  feature_importances_\n |      Return the feature importances.\n |      \n |      The importance of a feature is computed as the (normalized) total\n |      reduction of the criterion brought by that feature.\n |      It is also known as the Gini importance.\n |      \n |      Warning: impurity-based feature importances can be misleading for\n |      high cardinality features (many unique values). See\n |      :func:`sklearn.inspection.permutation_importance` as an alternative.\n |      \n |      Returns\n |      -------\n |      feature_importances_ : ndarray of shape (n_features,)\n |          Normalized total reduction of criteria by feature\n |          (Gini importance).\n |  \n |  ----------------------------------------------------------------------\n |  Methods inherited from sklearn.base.BaseEstimator:\n |  \n |  __getstate__(self)\n |  \n |  __repr__(self, N_CHAR_MAX=700)\n |      Return repr(self).\n |  \n |  __setstate__(self, state)\n |  \n |  get_params(self, deep=True)\n |      Get parameters for this estimator.\n |      \n |      Parameters\n |      ----------\n |      deep : bool, default=True\n |          If True, will return the parameters for this estimator and\n |          contained subobjects that are estimators.\n |      \n |      Returns\n |      -------\n |      params : dict\n |          Parameter names mapped to their values.\n |  \n |  set_params(self, **params)\n |      Set the parameters of this estimator.\n |      \n |      The method works on simple estimators as well as on nested objects\n |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n |      parameters of the form ``<component>__<parameter>`` so that it's\n |      possible to update each component of a nested object.\n |      \n |      Parameters\n |      ----------\n |      **params : dict\n |          Estimator parameters.\n |      \n |      Returns\n |      -------\n |      self : estimator instance\n |          Estimator instance.\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"One handy feature or this class is that it provides a function for drawing a tree representing the rules:","metadata":{}},{"cell_type":"code","source":"import graphviz\n\ndef draw_tree(t, df, size=10, ratio=0.6, precision=2, **kwargs):\n    s=export_graphviz(t, out_file=None, feature_names=df.columns, filled=True, rounded=True,\n                      special_characters=True, rotate=False, precision=precision, **kwargs)\n    return graphviz.Source(re.sub('Tree {', f'Tree {{ size={size}; ratio={ratio}', s))","metadata":{"execution":{"iopub.status.busy":"2022-11-01T10:49:27.472871Z","iopub.execute_input":"2022-11-01T10:49:27.473135Z","iopub.status.idle":"2022-11-01T10:49:27.488313Z","shell.execute_reply.started":"2022-11-01T10:49:27.473104Z","shell.execute_reply":"2022-11-01T10:49:27.487664Z"},"trusted":true},"execution_count":59,"outputs":[]},{"cell_type":"code","source":"draw_tree(m, trn_xs, size=10)\n\n# Lower gini is better\n# Gini calculates accuracy with data not trained on - should be low","metadata":{"execution":{"iopub.status.busy":"2022-11-01T10:51:09.097550Z","iopub.execute_input":"2022-11-01T10:51:09.097882Z","iopub.status.idle":"2022-11-01T10:51:09.142131Z","shell.execute_reply.started":"2022-11-01T10:51:09.097848Z","shell.execute_reply":"2022-11-01T10:51:09.141226Z"},"trusted":true},"execution_count":63,"outputs":[{"execution_count":63,"output_type":"execute_result","data":{"text/plain":"<graphviz.files.Source at 0x7f34c309b410>","image/svg+xml":"<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 2.50.0 (20211209.0339)\n -->\n<!-- Title: Tree Pages: 1 -->\n<svg width=\"593pt\" height=\"358pt\"\n viewBox=\"0.00 0.00 592.50 358.40\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 354.4)\">\n<title>Tree</title>\n<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-354.4 588.5,-354.4 588.5,4 -4,4\"/>\n<!-- 0 -->\n<g id=\"node1\" class=\"node\">\n<title>0</title>\n<path fill=\"#f5ceb2\" stroke=\"black\" d=\"M352,-339C352,-339 227,-339 227,-339 221,-339 215,-333 215,-327 215,-327 215,-283 215,-283 215,-277 221,-271 227,-271 227,-271 352,-271 352,-271 358,-271 364,-277 364,-283 364,-283 364,-327 364,-327 364,-333 358,-339 352,-339\"/>\n<text text-anchor=\"start\" x=\"254.5\" y=\"-323.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">Sex ≤ 0.5</text>\n<text text-anchor=\"start\" x=\"250\" y=\"-308.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.47</text>\n<text text-anchor=\"start\" x=\"236.5\" y=\"-293.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 668</text>\n<text text-anchor=\"start\" x=\"223\" y=\"-278.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [415, 253]</text>\n</g>\n<!-- 1 -->\n<g id=\"node2\" class=\"node\">\n<title>1</title>\n<path fill=\"#7ebfee\" stroke=\"black\" d=\"M268.5,-199C268.5,-199 152.5,-199 152.5,-199 146.5,-199 140.5,-193 140.5,-187 140.5,-187 140.5,-143 140.5,-143 140.5,-137 146.5,-131 152.5,-131 152.5,-131 268.5,-131 268.5,-131 274.5,-131 280.5,-137 280.5,-143 280.5,-143 280.5,-187 280.5,-187 280.5,-193 274.5,-199 268.5,-199\"/>\n<text text-anchor=\"start\" x=\"167\" y=\"-183.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">Pclass ≤ 2.5</text>\n<text text-anchor=\"start\" x=\"171\" y=\"-168.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.38</text>\n<text text-anchor=\"start\" x=\"157.5\" y=\"-153.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 229</text>\n<text text-anchor=\"start\" x=\"148.5\" y=\"-138.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [59, 170]</text>\n</g>\n<!-- 0&#45;&gt;1 -->\n<g id=\"edge1\" class=\"edge\">\n<title>0&#45;&gt;1</title>\n<path fill=\"none\" stroke=\"black\" d=\"M270.58,-270.95C259.76,-252.05 246.09,-228.17 234.52,-207.96\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"237.55,-206.2 229.54,-199.26 231.47,-209.68 237.55,-206.2\"/>\n<text text-anchor=\"middle\" x=\"222.98\" y=\"-219.69\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">True</text>\n</g>\n<!-- 2 -->\n<g id=\"node5\" class=\"node\">\n<title>2</title>\n<path fill=\"#eb9e67\" stroke=\"black\" d=\"M426.5,-199C426.5,-199 310.5,-199 310.5,-199 304.5,-199 298.5,-193 298.5,-187 298.5,-187 298.5,-143 298.5,-143 298.5,-137 304.5,-131 310.5,-131 310.5,-131 426.5,-131 426.5,-131 432.5,-131 438.5,-137 438.5,-143 438.5,-143 438.5,-187 438.5,-187 438.5,-193 432.5,-199 426.5,-199\"/>\n<text text-anchor=\"start\" x=\"333\" y=\"-183.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">Age ≤ 6.5</text>\n<text text-anchor=\"start\" x=\"329\" y=\"-168.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.31</text>\n<text text-anchor=\"start\" x=\"315.5\" y=\"-153.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 439</text>\n<text text-anchor=\"start\" x=\"306.5\" y=\"-138.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [356, 83]</text>\n</g>\n<!-- 0&#45;&gt;2 -->\n<g id=\"edge4\" class=\"edge\">\n<title>0&#45;&gt;2</title>\n<path fill=\"none\" stroke=\"black\" d=\"M308.42,-270.95C319.24,-252.05 332.91,-228.17 344.48,-207.96\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"347.53,-209.68 349.46,-199.26 341.45,-206.2 347.53,-209.68\"/>\n<text text-anchor=\"middle\" x=\"356.02\" y=\"-219.69\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">False</text>\n</g>\n<!-- 3 -->\n<g id=\"node3\" class=\"node\">\n<title>3</title>\n<path fill=\"#40a0e6\" stroke=\"black\" d=\"M119,-62.5C119,-62.5 12,-62.5 12,-62.5 6,-62.5 0,-56.5 0,-50.5 0,-50.5 0,-21.5 0,-21.5 0,-15.5 6,-9.5 12,-9.5 12,-9.5 119,-9.5 119,-9.5 125,-9.5 131,-15.5 131,-21.5 131,-21.5 131,-50.5 131,-50.5 131,-56.5 125,-62.5 119,-62.5\"/>\n<text text-anchor=\"start\" x=\"26\" y=\"-47.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.06</text>\n<text text-anchor=\"start\" x=\"12.5\" y=\"-32.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 120</text>\n<text text-anchor=\"start\" x=\"8\" y=\"-17.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [4, 116]</text>\n</g>\n<!-- 1&#45;&gt;3 -->\n<g id=\"edge2\" class=\"edge\">\n<title>1&#45;&gt;3</title>\n<path fill=\"none\" stroke=\"black\" d=\"M172.78,-130.96C151.11,-111.98 124,-88.24 102.46,-69.37\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"104.62,-66.61 94.79,-62.66 100.01,-71.88 104.62,-66.61\"/>\n</g>\n<!-- 4 -->\n<g id=\"node4\" class=\"node\">\n<title>4</title>\n<path fill=\"#fffdfb\" stroke=\"black\" d=\"M268,-62.5C268,-62.5 161,-62.5 161,-62.5 155,-62.5 149,-56.5 149,-50.5 149,-50.5 149,-21.5 149,-21.5 149,-15.5 155,-9.5 161,-9.5 161,-9.5 268,-9.5 268,-9.5 274,-9.5 280,-15.5 280,-21.5 280,-21.5 280,-50.5 280,-50.5 280,-56.5 274,-62.5 268,-62.5\"/>\n<text text-anchor=\"start\" x=\"179.5\" y=\"-47.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.5</text>\n<text text-anchor=\"start\" x=\"161.5\" y=\"-32.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 109</text>\n<text text-anchor=\"start\" x=\"157\" y=\"-17.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [55, 54]</text>\n</g>\n<!-- 1&#45;&gt;4 -->\n<g id=\"edge3\" class=\"edge\">\n<title>1&#45;&gt;4</title>\n<path fill=\"none\" stroke=\"black\" d=\"M211.54,-130.96C212.1,-113.15 212.79,-91.14 213.37,-72.9\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"216.88,-72.76 213.69,-62.66 209.88,-72.54 216.88,-72.76\"/>\n</g>\n<!-- 5 -->\n<g id=\"node6\" class=\"node\">\n<title>5</title>\n<path fill=\"#88c4ef\" stroke=\"black\" d=\"M414.5,-62.5C414.5,-62.5 316.5,-62.5 316.5,-62.5 310.5,-62.5 304.5,-56.5 304.5,-50.5 304.5,-50.5 304.5,-21.5 304.5,-21.5 304.5,-15.5 310.5,-9.5 316.5,-9.5 316.5,-9.5 414.5,-9.5 414.5,-9.5 420.5,-9.5 426.5,-15.5 426.5,-21.5 426.5,-21.5 426.5,-50.5 426.5,-50.5 426.5,-56.5 420.5,-62.5 414.5,-62.5\"/>\n<text text-anchor=\"start\" x=\"326\" y=\"-47.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.41</text>\n<text text-anchor=\"start\" x=\"317\" y=\"-32.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 21</text>\n<text text-anchor=\"start\" x=\"312.5\" y=\"-17.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [6, 15]</text>\n</g>\n<!-- 2&#45;&gt;5 -->\n<g id=\"edge5\" class=\"edge\">\n<title>2&#45;&gt;5</title>\n<path fill=\"none\" stroke=\"black\" d=\"M367.72,-130.96C367.3,-113.15 366.78,-91.14 366.35,-72.9\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"369.84,-72.57 366.11,-62.66 362.84,-72.74 369.84,-72.57\"/>\n</g>\n<!-- 6 -->\n<g id=\"node7\" class=\"node\">\n<title>6</title>\n<path fill=\"#ea995f\" stroke=\"black\" d=\"M572.5,-62.5C572.5,-62.5 456.5,-62.5 456.5,-62.5 450.5,-62.5 444.5,-56.5 444.5,-50.5 444.5,-50.5 444.5,-21.5 444.5,-21.5 444.5,-15.5 450.5,-9.5 456.5,-9.5 456.5,-9.5 572.5,-9.5 572.5,-9.5 578.5,-9.5 584.5,-15.5 584.5,-21.5 584.5,-21.5 584.5,-50.5 584.5,-50.5 584.5,-56.5 578.5,-62.5 572.5,-62.5\"/>\n<text text-anchor=\"start\" x=\"475\" y=\"-47.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.27</text>\n<text text-anchor=\"start\" x=\"461.5\" y=\"-32.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 418</text>\n<text text-anchor=\"start\" x=\"452.5\" y=\"-17.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [350, 68]</text>\n</g>\n<!-- 2&#45;&gt;6 -->\n<g id=\"edge6\" class=\"edge\">\n<title>2&#45;&gt;6</title>\n<path fill=\"none\" stroke=\"black\" d=\"M406.48,-130.96C428.3,-111.98 455.6,-88.24 477.29,-69.37\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"479.76,-71.86 485.01,-62.66 475.16,-66.58 479.76,-71.86\"/>\n</g>\n</g>\n</svg>\n"},"metadata":{}}]},{"cell_type":"markdown","source":"We can see that it's found exactly the same splits as we did!\n\nIn this picture, the more orange nodes have a lower survival rate, and blue have higher survival. Each node shows how many rows (\"*samples*\") match that set of rules, and shows how many perish or survive (\"*values*\"). There's also something called \"*gini*\". That's another measure of impurity, and it's very similar to the `score()` we created earlier. It's defined as follows:","metadata":{}},{"cell_type":"code","source":"def gini(cond):\n    act = df.loc[cond, dep]\n    return 1 - act.mean()**2 - (1-act).mean()**2","metadata":{"execution":{"iopub.status.busy":"2022-11-01T10:51:28.892751Z","iopub.execute_input":"2022-11-01T10:51:28.893061Z","iopub.status.idle":"2022-11-01T10:51:28.898326Z","shell.execute_reply.started":"2022-11-01T10:51:28.893013Z","shell.execute_reply":"2022-11-01T10:51:28.897465Z"},"trusted":true},"execution_count":64,"outputs":[]},{"cell_type":"code","source":"dep","metadata":{"execution":{"iopub.status.busy":"2022-11-01T10:51:46.176936Z","iopub.execute_input":"2022-11-01T10:51:46.177354Z","iopub.status.idle":"2022-11-01T10:51:46.185302Z","shell.execute_reply.started":"2022-11-01T10:51:46.177310Z","shell.execute_reply":"2022-11-01T10:51:46.184597Z"},"trusted":true},"execution_count":66,"outputs":[{"execution_count":66,"output_type":"execute_result","data":{"text/plain":"'Survived'"},"metadata":{}}]},{"cell_type":"markdown","source":"What this calculates is the probability that, if you pick two rows from a group, you'll get the same `Survived` result each time. If the group is all the same, the probability is `1.0`, and `0.0` if they're all different:","metadata":{}},{"cell_type":"code","source":"gini(df.Sex=='female'), gini(df.Sex=='male')\n# Lower for male, so males more likely to not survive","metadata":{"execution":{"iopub.status.busy":"2022-11-01T10:52:10.163556Z","iopub.execute_input":"2022-11-01T10:52:10.164042Z","iopub.status.idle":"2022-11-01T10:52:10.180776Z","shell.execute_reply.started":"2022-11-01T10:52:10.164003Z","shell.execute_reply":"2022-11-01T10:52:10.179972Z"},"trusted":true},"execution_count":67,"outputs":[{"execution_count":67,"output_type":"execute_result","data":{"text/plain":"(0.3828350034484158, 0.3064437162277842)"},"metadata":{}}]},{"cell_type":"markdown","source":"Let's see how this model compares to our OneR version:","metadata":{}},{"cell_type":"code","source":"mean_absolute_error(val_y, m.predict(val_xs))","metadata":{"execution":{"iopub.status.busy":"2022-11-01T10:52:44.228515Z","iopub.execute_input":"2022-11-01T10:52:44.228806Z","iopub.status.idle":"2022-11-01T10:52:44.237913Z","shell.execute_reply.started":"2022-11-01T10:52:44.228774Z","shell.execute_reply":"2022-11-01T10:52:44.236999Z"},"trusted":true},"execution_count":69,"outputs":[{"execution_count":69,"output_type":"execute_result","data":{"text/plain":"0.2242152466367713"},"metadata":{}}]},{"cell_type":"markdown","source":"It's a tiny bit worse. Since this is such a small dataset (we've only got around 200 rows in our validation set) this small difference isn't really meaningful. Perhaps we'll see better results if we create a bigger tree:","metadata":{}},{"cell_type":"code","source":"m = DecisionTreeClassifier(min_samples_leaf=50)\nm.fit(trn_xs, trn_y)\ndraw_tree(m, trn_xs, size=12)","metadata":{"execution":{"iopub.status.busy":"2022-11-01T10:52:49.555909Z","iopub.execute_input":"2022-11-01T10:52:49.556877Z","iopub.status.idle":"2022-11-01T10:52:49.610710Z","shell.execute_reply.started":"2022-11-01T10:52:49.556825Z","shell.execute_reply":"2022-11-01T10:52:49.609879Z"},"trusted":true},"execution_count":70,"outputs":[{"execution_count":70,"output_type":"execute_result","data":{"text/plain":"<graphviz.files.Source at 0x7f34c32f7650>","image/svg+xml":"<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 2.50.0 (20211209.0339)\n -->\n<!-- Title: Tree Pages: 1 -->\n<svg width=\"864pt\" height=\"521pt\"\n viewBox=\"0.00 0.00 864.00 520.53\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(0.74 0.74) rotate(0) translate(4 703.6)\">\n<title>Tree</title>\n<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-703.6 1170.5,-703.6 1170.5,4 -4,4\"/>\n<!-- 0 -->\n<g id=\"node1\" class=\"node\">\n<title>0</title>\n<path fill=\"#f5ceb2\" stroke=\"black\" d=\"M627.5,-698C627.5,-698 502.5,-698 502.5,-698 496.5,-698 490.5,-692 490.5,-686 490.5,-686 490.5,-642 490.5,-642 490.5,-636 496.5,-630 502.5,-630 502.5,-630 627.5,-630 627.5,-630 633.5,-630 639.5,-636 639.5,-642 639.5,-642 639.5,-686 639.5,-686 639.5,-692 633.5,-698 627.5,-698\"/>\n<text text-anchor=\"start\" x=\"530\" y=\"-682.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">Sex ≤ 0.5</text>\n<text text-anchor=\"start\" x=\"525.5\" y=\"-667.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.47</text>\n<text text-anchor=\"start\" x=\"512\" y=\"-652.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 668</text>\n<text text-anchor=\"start\" x=\"498.5\" y=\"-637.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [415, 253]</text>\n</g>\n<!-- 1 -->\n<g id=\"node2\" class=\"node\">\n<title>1</title>\n<path fill=\"#7ebfee\" stroke=\"black\" d=\"M476,-591C476,-591 360,-591 360,-591 354,-591 348,-585 348,-579 348,-579 348,-535 348,-535 348,-529 354,-523 360,-523 360,-523 476,-523 476,-523 482,-523 488,-529 488,-535 488,-535 488,-579 488,-579 488,-585 482,-591 476,-591\"/>\n<text text-anchor=\"start\" x=\"374.5\" y=\"-575.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">Pclass ≤ 2.5</text>\n<text text-anchor=\"start\" x=\"378.5\" y=\"-560.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.38</text>\n<text text-anchor=\"start\" x=\"365\" y=\"-545.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 229</text>\n<text text-anchor=\"start\" x=\"356\" y=\"-530.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [59, 170]</text>\n</g>\n<!-- 0&#45;&gt;1 -->\n<g id=\"edge1\" class=\"edge\">\n<title>0&#45;&gt;1</title>\n<path fill=\"none\" stroke=\"black\" d=\"M518.49,-629.78C503.87,-619.33 487.59,-607.71 472.47,-596.91\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"474.38,-593.97 464.21,-591.01 470.31,-599.67 474.38,-593.97\"/>\n<text text-anchor=\"middle\" x=\"468.32\" y=\"-611.97\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">True</text>\n</g>\n<!-- 8 -->\n<g id=\"node9\" class=\"node\">\n<title>8</title>\n<path fill=\"#eb9e67\" stroke=\"black\" d=\"M779,-591C779,-591 663,-591 663,-591 657,-591 651,-585 651,-579 651,-579 651,-535 651,-535 651,-529 657,-523 663,-523 663,-523 779,-523 779,-523 785,-523 791,-529 791,-535 791,-535 791,-579 791,-579 791,-585 785,-591 779,-591\"/>\n<text text-anchor=\"start\" x=\"668\" y=\"-575.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">LogFare ≤ 3.31</text>\n<text text-anchor=\"start\" x=\"681.5\" y=\"-560.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.31</text>\n<text text-anchor=\"start\" x=\"668\" y=\"-545.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 439</text>\n<text text-anchor=\"start\" x=\"659\" y=\"-530.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [356, 83]</text>\n</g>\n<!-- 0&#45;&gt;8 -->\n<g id=\"edge8\" class=\"edge\">\n<title>0&#45;&gt;8</title>\n<path fill=\"none\" stroke=\"black\" d=\"M614.36,-629.78C630.02,-619.24 647.47,-607.49 663.64,-596.61\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"665.62,-599.49 671.96,-591.01 661.71,-593.69 665.62,-599.49\"/>\n<text text-anchor=\"middle\" x=\"667.17\" y=\"-611.84\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">False</text>\n</g>\n<!-- 2 -->\n<g id=\"node3\" class=\"node\">\n<title>2</title>\n<path fill=\"#40a0e6\" stroke=\"black\" d=\"M254.5,-484C254.5,-484 147.5,-484 147.5,-484 141.5,-484 135.5,-478 135.5,-472 135.5,-472 135.5,-428 135.5,-428 135.5,-422 141.5,-416 147.5,-416 147.5,-416 254.5,-416 254.5,-416 260.5,-416 266.5,-422 266.5,-428 266.5,-428 266.5,-472 266.5,-472 266.5,-478 260.5,-484 254.5,-484\"/>\n<text text-anchor=\"start\" x=\"159\" y=\"-468.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">SibSp ≤ 0.5</text>\n<text text-anchor=\"start\" x=\"161.5\" y=\"-453.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.06</text>\n<text text-anchor=\"start\" x=\"148\" y=\"-438.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 120</text>\n<text text-anchor=\"start\" x=\"143.5\" y=\"-423.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [4, 116]</text>\n</g>\n<!-- 1&#45;&gt;2 -->\n<g id=\"edge2\" class=\"edge\">\n<title>1&#45;&gt;2</title>\n<path fill=\"none\" stroke=\"black\" d=\"M349.64,-522.92C326.17,-511.57 299.79,-498.8 275.84,-487.21\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"277.32,-484.04 266.79,-482.84 274.27,-490.34 277.32,-484.04\"/>\n</g>\n<!-- 5 -->\n<g id=\"node6\" class=\"node\">\n<title>5</title>\n<path fill=\"#fffdfb\" stroke=\"black\" d=\"M471.5,-484C471.5,-484 364.5,-484 364.5,-484 358.5,-484 352.5,-478 352.5,-472 352.5,-472 352.5,-428 352.5,-428 352.5,-422 358.5,-416 364.5,-416 364.5,-416 471.5,-416 471.5,-416 477.5,-416 483.5,-422 483.5,-428 483.5,-428 483.5,-472 483.5,-472 483.5,-478 477.5,-484 471.5,-484\"/>\n<text text-anchor=\"start\" x=\"369.5\" y=\"-468.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">LogFare ≤ 2.7</text>\n<text text-anchor=\"start\" x=\"383\" y=\"-453.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.5</text>\n<text text-anchor=\"start\" x=\"365\" y=\"-438.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 109</text>\n<text text-anchor=\"start\" x=\"360.5\" y=\"-423.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [55, 54]</text>\n</g>\n<!-- 1&#45;&gt;5 -->\n<g id=\"edge5\" class=\"edge\">\n<title>1&#45;&gt;5</title>\n<path fill=\"none\" stroke=\"black\" d=\"M418,-522.78C418,-513.69 418,-503.7 418,-494.15\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"421.5,-494.01 418,-484.01 414.5,-494.01 421.5,-494.01\"/>\n</g>\n<!-- 3 -->\n<g id=\"node4\" class=\"node\">\n<title>3</title>\n<path fill=\"#3c9ee5\" stroke=\"black\" d=\"M110,-368.5C110,-368.5 12,-368.5 12,-368.5 6,-368.5 0,-362.5 0,-356.5 0,-356.5 0,-327.5 0,-327.5 0,-321.5 6,-315.5 12,-315.5 12,-315.5 110,-315.5 110,-315.5 116,-315.5 122,-321.5 122,-327.5 122,-327.5 122,-356.5 122,-356.5 122,-362.5 116,-368.5 110,-368.5\"/>\n<text text-anchor=\"start\" x=\"21.5\" y=\"-353.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.03</text>\n<text text-anchor=\"start\" x=\"12.5\" y=\"-338.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 67</text>\n<text text-anchor=\"start\" x=\"8\" y=\"-323.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [1, 66]</text>\n</g>\n<!-- 2&#45;&gt;3 -->\n<g id=\"edge3\" class=\"edge\">\n<title>2&#45;&gt;3</title>\n<path fill=\"none\" stroke=\"black\" d=\"M157.09,-415.75C139.86,-402.71 120.17,-387.8 103.12,-374.89\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"105.07,-371.98 94.98,-368.73 100.84,-377.56 105.07,-371.98\"/>\n</g>\n<!-- 4 -->\n<g id=\"node5\" class=\"node\">\n<title>4</title>\n<path fill=\"#45a3e7\" stroke=\"black\" d=\"M250,-368.5C250,-368.5 152,-368.5 152,-368.5 146,-368.5 140,-362.5 140,-356.5 140,-356.5 140,-327.5 140,-327.5 140,-321.5 146,-315.5 152,-315.5 152,-315.5 250,-315.5 250,-315.5 256,-315.5 262,-321.5 262,-327.5 262,-327.5 262,-356.5 262,-356.5 262,-362.5 256,-368.5 250,-368.5\"/>\n<text text-anchor=\"start\" x=\"161.5\" y=\"-353.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.11</text>\n<text text-anchor=\"start\" x=\"152.5\" y=\"-338.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 53</text>\n<text text-anchor=\"start\" x=\"148\" y=\"-323.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [3, 50]</text>\n</g>\n<!-- 2&#45;&gt;4 -->\n<g id=\"edge4\" class=\"edge\">\n<title>2&#45;&gt;4</title>\n<path fill=\"none\" stroke=\"black\" d=\"M201,-415.75C201,-404.01 201,-390.76 201,-378.82\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"204.5,-378.73 201,-368.73 197.5,-378.73 204.5,-378.73\"/>\n</g>\n<!-- 6 -->\n<g id=\"node7\" class=\"node\">\n<title>6</title>\n<path fill=\"#cbe5f8\" stroke=\"black\" d=\"M399.5,-368.5C399.5,-368.5 292.5,-368.5 292.5,-368.5 286.5,-368.5 280.5,-362.5 280.5,-356.5 280.5,-356.5 280.5,-327.5 280.5,-327.5 280.5,-321.5 286.5,-315.5 292.5,-315.5 292.5,-315.5 399.5,-315.5 399.5,-315.5 405.5,-315.5 411.5,-321.5 411.5,-327.5 411.5,-327.5 411.5,-356.5 411.5,-356.5 411.5,-362.5 405.5,-368.5 399.5,-368.5\"/>\n<text text-anchor=\"start\" x=\"306.5\" y=\"-353.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.49</text>\n<text text-anchor=\"start\" x=\"297.5\" y=\"-338.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 59</text>\n<text text-anchor=\"start\" x=\"288.5\" y=\"-323.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [25, 34]</text>\n</g>\n<!-- 5&#45;&gt;6 -->\n<g id=\"edge6\" class=\"edge\">\n<title>5&#45;&gt;6</title>\n<path fill=\"none\" stroke=\"black\" d=\"M395.42,-415.75C387.04,-403.42 377.53,-389.42 369.11,-377.02\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"371.99,-375.04 363.48,-368.73 366.2,-378.97 371.99,-375.04\"/>\n</g>\n<!-- 7 -->\n<g id=\"node8\" class=\"node\">\n<title>7</title>\n<path fill=\"#f6d5bd\" stroke=\"black\" d=\"M548.5,-368.5C548.5,-368.5 441.5,-368.5 441.5,-368.5 435.5,-368.5 429.5,-362.5 429.5,-356.5 429.5,-356.5 429.5,-327.5 429.5,-327.5 429.5,-321.5 435.5,-315.5 441.5,-315.5 441.5,-315.5 548.5,-315.5 548.5,-315.5 554.5,-315.5 560.5,-321.5 560.5,-327.5 560.5,-327.5 560.5,-356.5 560.5,-356.5 560.5,-362.5 554.5,-368.5 548.5,-368.5\"/>\n<text text-anchor=\"start\" x=\"455.5\" y=\"-353.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.48</text>\n<text text-anchor=\"start\" x=\"446.5\" y=\"-338.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 50</text>\n<text text-anchor=\"start\" x=\"437.5\" y=\"-323.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [30, 20]</text>\n</g>\n<!-- 5&#45;&gt;7 -->\n<g id=\"edge7\" class=\"edge\">\n<title>5&#45;&gt;7</title>\n<path fill=\"none\" stroke=\"black\" d=\"M442.15,-415.75C451.11,-403.42 461.28,-389.42 470.29,-377.02\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"473.26,-378.88 476.31,-368.73 467.6,-374.76 473.26,-378.88\"/>\n</g>\n<!-- 9 -->\n<g id=\"node10\" class=\"node\">\n<title>9</title>\n<path fill=\"#e99659\" stroke=\"black\" d=\"M779,-484C779,-484 663,-484 663,-484 657,-484 651,-478 651,-472 651,-472 651,-428 651,-428 651,-422 657,-416 663,-416 663,-416 779,-416 779,-416 785,-416 791,-422 791,-428 791,-428 791,-472 791,-472 791,-478 785,-484 779,-484\"/>\n<text text-anchor=\"start\" x=\"681\" y=\"-468.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">Age ≤ 20.5</text>\n<text text-anchor=\"start\" x=\"681.5\" y=\"-453.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.24</text>\n<text text-anchor=\"start\" x=\"668\" y=\"-438.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 320</text>\n<text text-anchor=\"start\" x=\"659\" y=\"-423.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [275, 45]</text>\n</g>\n<!-- 8&#45;&gt;9 -->\n<g id=\"edge9\" class=\"edge\">\n<title>8&#45;&gt;9</title>\n<path fill=\"none\" stroke=\"black\" d=\"M721,-522.78C721,-513.69 721,-503.7 721,-494.15\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"724.5,-494.01 721,-484.01 717.5,-494.01 724.5,-494.01\"/>\n</g>\n<!-- 18 -->\n<g id=\"node19\" class=\"node\">\n<title>18</title>\n<path fill=\"#f1bc96\" stroke=\"black\" d=\"M1005.5,-484C1005.5,-484 898.5,-484 898.5,-484 892.5,-484 886.5,-478 886.5,-472 886.5,-472 886.5,-428 886.5,-428 886.5,-422 892.5,-416 898.5,-416 898.5,-416 1005.5,-416 1005.5,-416 1011.5,-416 1017.5,-422 1017.5,-428 1017.5,-428 1017.5,-472 1017.5,-472 1017.5,-478 1011.5,-484 1005.5,-484\"/>\n<text text-anchor=\"start\" x=\"910\" y=\"-468.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">SibSp ≤ 0.5</text>\n<text text-anchor=\"start\" x=\"912.5\" y=\"-453.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.43</text>\n<text text-anchor=\"start\" x=\"899\" y=\"-438.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 119</text>\n<text text-anchor=\"start\" x=\"894.5\" y=\"-423.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [81, 38]</text>\n</g>\n<!-- 8&#45;&gt;18 -->\n<g id=\"edge18\" class=\"edge\">\n<title>8&#45;&gt;18</title>\n<path fill=\"none\" stroke=\"black\" d=\"M791.25,-524.07C818.44,-511.71 849.6,-497.54 877.21,-485\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"878.77,-488.13 886.42,-480.81 875.87,-481.76 878.77,-488.13\"/>\n</g>\n<!-- 10 -->\n<g id=\"node11\" class=\"node\">\n<title>10</title>\n<path fill=\"#f1b992\" stroke=\"black\" d=\"M697.5,-368.5C697.5,-368.5 590.5,-368.5 590.5,-368.5 584.5,-368.5 578.5,-362.5 578.5,-356.5 578.5,-356.5 578.5,-327.5 578.5,-327.5 578.5,-321.5 584.5,-315.5 590.5,-315.5 590.5,-315.5 697.5,-315.5 697.5,-315.5 703.5,-315.5 709.5,-321.5 709.5,-327.5 709.5,-327.5 709.5,-356.5 709.5,-356.5 709.5,-362.5 703.5,-368.5 697.5,-368.5\"/>\n<text text-anchor=\"start\" x=\"604.5\" y=\"-353.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.43</text>\n<text text-anchor=\"start\" x=\"595.5\" y=\"-338.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 55</text>\n<text text-anchor=\"start\" x=\"586.5\" y=\"-323.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [38, 17]</text>\n</g>\n<!-- 9&#45;&gt;10 -->\n<g id=\"edge10\" class=\"edge\">\n<title>9&#45;&gt;10</title>\n<path fill=\"none\" stroke=\"black\" d=\"M696.85,-415.75C687.89,-403.42 677.72,-389.42 668.71,-377.02\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"671.4,-374.76 662.69,-368.73 665.74,-378.88 671.4,-374.76\"/>\n</g>\n<!-- 11 -->\n<g id=\"node12\" class=\"node\">\n<title>11</title>\n<path fill=\"#e89050\" stroke=\"black\" d=\"M856,-376C856,-376 740,-376 740,-376 734,-376 728,-370 728,-364 728,-364 728,-320 728,-320 728,-314 734,-308 740,-308 740,-308 856,-308 856,-308 862,-308 868,-314 868,-320 868,-320 868,-364 868,-364 868,-370 862,-376 856,-376\"/>\n<text text-anchor=\"start\" x=\"758\" y=\"-360.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">Age ≤ 32.5</text>\n<text text-anchor=\"start\" x=\"758.5\" y=\"-345.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.19</text>\n<text text-anchor=\"start\" x=\"745\" y=\"-330.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 265</text>\n<text text-anchor=\"start\" x=\"736\" y=\"-315.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [237, 28]</text>\n</g>\n<!-- 9&#45;&gt;11 -->\n<g id=\"edge11\" class=\"edge\">\n<title>9&#45;&gt;11</title>\n<path fill=\"none\" stroke=\"black\" d=\"M745.15,-415.75C752.39,-405.79 760.42,-394.73 767.99,-384.32\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"770.89,-386.28 773.93,-376.13 765.22,-382.16 770.89,-386.28\"/>\n</g>\n<!-- 12 -->\n<g id=\"node13\" class=\"node\">\n<title>12</title>\n<path fill=\"#e99356\" stroke=\"black\" d=\"M781,-269C781,-269 665,-269 665,-269 659,-269 653,-263 653,-257 653,-257 653,-213 653,-213 653,-207 659,-201 665,-201 665,-201 781,-201 781,-201 787,-201 793,-207 793,-213 793,-213 793,-257 793,-257 793,-263 787,-269 781,-269\"/>\n<text text-anchor=\"start\" x=\"678.5\" y=\"-253.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">Age ≤ 24.75</text>\n<text text-anchor=\"start\" x=\"683.5\" y=\"-238.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.22</text>\n<text text-anchor=\"start\" x=\"670\" y=\"-223.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 181</text>\n<text text-anchor=\"start\" x=\"661\" y=\"-208.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [158, 23]</text>\n</g>\n<!-- 11&#45;&gt;12 -->\n<g id=\"edge12\" class=\"edge\">\n<title>11&#45;&gt;12</title>\n<path fill=\"none\" stroke=\"black\" d=\"M774.27,-307.78C767.36,-298.11 759.73,-287.43 752.52,-277.32\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"755.24,-275.11 746.58,-269.01 749.54,-279.18 755.24,-275.11\"/>\n</g>\n<!-- 17 -->\n<g id=\"node18\" class=\"node\">\n<title>17</title>\n<path fill=\"#e78946\" stroke=\"black\" d=\"M921,-261.5C921,-261.5 823,-261.5 823,-261.5 817,-261.5 811,-255.5 811,-249.5 811,-249.5 811,-220.5 811,-220.5 811,-214.5 817,-208.5 823,-208.5 823,-208.5 921,-208.5 921,-208.5 927,-208.5 933,-214.5 933,-220.5 933,-220.5 933,-249.5 933,-249.5 933,-255.5 927,-261.5 921,-261.5\"/>\n<text text-anchor=\"start\" x=\"832.5\" y=\"-246.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.11</text>\n<text text-anchor=\"start\" x=\"823.5\" y=\"-231.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 84</text>\n<text text-anchor=\"start\" x=\"819\" y=\"-216.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [79, 5]</text>\n</g>\n<!-- 11&#45;&gt;17 -->\n<g id=\"edge17\" class=\"edge\">\n<title>11&#45;&gt;17</title>\n<path fill=\"none\" stroke=\"black\" d=\"M821.41,-307.78C829.86,-295.79 839.41,-282.24 847.92,-270.17\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"851,-271.88 853.9,-261.69 845.28,-267.84 851,-271.88\"/>\n</g>\n<!-- 13 -->\n<g id=\"node14\" class=\"node\">\n<title>13</title>\n<path fill=\"#e88d4c\" stroke=\"black\" d=\"M704,-161C704,-161 588,-161 588,-161 582,-161 576,-155 576,-149 576,-149 576,-105 576,-105 576,-99 582,-93 588,-93 588,-93 704,-93 704,-93 710,-93 716,-99 716,-105 716,-105 716,-149 716,-149 716,-155 710,-161 704,-161\"/>\n<text text-anchor=\"start\" x=\"593\" y=\"-145.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">LogFare ≤ 2.18</text>\n<text text-anchor=\"start\" x=\"606.5\" y=\"-130.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.16</text>\n<text text-anchor=\"start\" x=\"593\" y=\"-115.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 114</text>\n<text text-anchor=\"start\" x=\"584\" y=\"-100.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [104, 10]</text>\n</g>\n<!-- 12&#45;&gt;13 -->\n<g id=\"edge13\" class=\"edge\">\n<title>12&#45;&gt;13</title>\n<path fill=\"none\" stroke=\"black\" d=\"M698.85,-200.75C691.61,-190.79 683.58,-179.73 676.01,-169.32\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"678.78,-167.16 670.07,-161.13 673.11,-171.28 678.78,-167.16\"/>\n</g>\n<!-- 16 -->\n<g id=\"node17\" class=\"node\">\n<title>16</title>\n<path fill=\"#eb9f69\" stroke=\"black\" d=\"M853.5,-153.5C853.5,-153.5 746.5,-153.5 746.5,-153.5 740.5,-153.5 734.5,-147.5 734.5,-141.5 734.5,-141.5 734.5,-112.5 734.5,-112.5 734.5,-106.5 740.5,-100.5 746.5,-100.5 746.5,-100.5 853.5,-100.5 853.5,-100.5 859.5,-100.5 865.5,-106.5 865.5,-112.5 865.5,-112.5 865.5,-141.5 865.5,-141.5 865.5,-147.5 859.5,-153.5 853.5,-153.5\"/>\n<text text-anchor=\"start\" x=\"760.5\" y=\"-138.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.31</text>\n<text text-anchor=\"start\" x=\"751.5\" y=\"-123.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 67</text>\n<text text-anchor=\"start\" x=\"742.5\" y=\"-108.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [54, 13]</text>\n</g>\n<!-- 12&#45;&gt;16 -->\n<g id=\"edge16\" class=\"edge\">\n<title>12&#45;&gt;16</title>\n<path fill=\"none\" stroke=\"black\" d=\"M747.15,-200.75C756.11,-188.42 766.28,-174.42 775.29,-162.02\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"778.26,-163.88 781.31,-153.73 772.6,-159.76 778.26,-163.88\"/>\n</g>\n<!-- 14 -->\n<g id=\"node15\" class=\"node\">\n<title>14</title>\n<path fill=\"#e99254\" stroke=\"black\" d=\"M625,-53.5C625,-53.5 527,-53.5 527,-53.5 521,-53.5 515,-47.5 515,-41.5 515,-41.5 515,-12.5 515,-12.5 515,-6.5 521,-0.5 527,-0.5 527,-0.5 625,-0.5 625,-0.5 631,-0.5 637,-6.5 637,-12.5 637,-12.5 637,-41.5 637,-41.5 637,-47.5 631,-53.5 625,-53.5\"/>\n<text text-anchor=\"start\" x=\"536.5\" y=\"-38.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.21</text>\n<text text-anchor=\"start\" x=\"527.5\" y=\"-23.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 50</text>\n<text text-anchor=\"start\" x=\"523\" y=\"-8.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [44, 6]</text>\n</g>\n<!-- 13&#45;&gt;14 -->\n<g id=\"edge14\" class=\"edge\">\n<title>13&#45;&gt;14</title>\n<path fill=\"none\" stroke=\"black\" d=\"M622.3,-92.82C615.17,-82.84 607.35,-71.89 600.19,-61.86\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"603.02,-59.81 594.36,-53.71 597.33,-63.88 603.02,-59.81\"/>\n</g>\n<!-- 15 -->\n<g id=\"node16\" class=\"node\">\n<title>15</title>\n<path fill=\"#e78946\" stroke=\"black\" d=\"M765,-53.5C765,-53.5 667,-53.5 667,-53.5 661,-53.5 655,-47.5 655,-41.5 655,-41.5 655,-12.5 655,-12.5 655,-6.5 661,-0.5 667,-0.5 667,-0.5 765,-0.5 765,-0.5 771,-0.5 777,-6.5 777,-12.5 777,-12.5 777,-41.5 777,-41.5 777,-47.5 771,-53.5 765,-53.5\"/>\n<text text-anchor=\"start\" x=\"676.5\" y=\"-38.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.12</text>\n<text text-anchor=\"start\" x=\"667.5\" y=\"-23.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 64</text>\n<text text-anchor=\"start\" x=\"663\" y=\"-8.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [60, 4]</text>\n</g>\n<!-- 13&#45;&gt;15 -->\n<g id=\"edge15\" class=\"edge\">\n<title>13&#45;&gt;15</title>\n<path fill=\"none\" stroke=\"black\" d=\"M669.7,-92.82C676.83,-82.84 684.65,-71.89 691.81,-61.86\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"694.67,-63.88 697.64,-53.71 688.98,-59.81 694.67,-63.88\"/>\n</g>\n<!-- 19 -->\n<g id=\"node20\" class=\"node\">\n<title>19</title>\n<path fill=\"#f6d5bd\" stroke=\"black\" d=\"M1005.5,-368.5C1005.5,-368.5 898.5,-368.5 898.5,-368.5 892.5,-368.5 886.5,-362.5 886.5,-356.5 886.5,-356.5 886.5,-327.5 886.5,-327.5 886.5,-321.5 892.5,-315.5 898.5,-315.5 898.5,-315.5 1005.5,-315.5 1005.5,-315.5 1011.5,-315.5 1017.5,-321.5 1017.5,-327.5 1017.5,-327.5 1017.5,-356.5 1017.5,-356.5 1017.5,-362.5 1011.5,-368.5 1005.5,-368.5\"/>\n<text text-anchor=\"start\" x=\"912.5\" y=\"-353.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.48</text>\n<text text-anchor=\"start\" x=\"903.5\" y=\"-338.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 60</text>\n<text text-anchor=\"start\" x=\"894.5\" y=\"-323.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [36, 24]</text>\n</g>\n<!-- 18&#45;&gt;19 -->\n<g id=\"edge19\" class=\"edge\">\n<title>18&#45;&gt;19</title>\n<path fill=\"none\" stroke=\"black\" d=\"M952,-415.75C952,-404.01 952,-390.76 952,-378.82\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"955.5,-378.73 952,-368.73 948.5,-378.73 955.5,-378.73\"/>\n</g>\n<!-- 20 -->\n<g id=\"node21\" class=\"node\">\n<title>20</title>\n<path fill=\"#eda877\" stroke=\"black\" d=\"M1154.5,-368.5C1154.5,-368.5 1047.5,-368.5 1047.5,-368.5 1041.5,-368.5 1035.5,-362.5 1035.5,-356.5 1035.5,-356.5 1035.5,-327.5 1035.5,-327.5 1035.5,-321.5 1041.5,-315.5 1047.5,-315.5 1047.5,-315.5 1154.5,-315.5 1154.5,-315.5 1160.5,-315.5 1166.5,-321.5 1166.5,-327.5 1166.5,-327.5 1166.5,-356.5 1166.5,-356.5 1166.5,-362.5 1160.5,-368.5 1154.5,-368.5\"/>\n<text text-anchor=\"start\" x=\"1061.5\" y=\"-353.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.36</text>\n<text text-anchor=\"start\" x=\"1052.5\" y=\"-338.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 59</text>\n<text text-anchor=\"start\" x=\"1043.5\" y=\"-323.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [45, 14]</text>\n</g>\n<!-- 18&#45;&gt;20 -->\n<g id=\"edge20\" class=\"edge\">\n<title>18&#45;&gt;20</title>\n<path fill=\"none\" stroke=\"black\" d=\"M998.74,-415.75C1017.24,-402.59 1038.41,-387.53 1056.67,-374.54\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"1058.71,-377.38 1064.83,-368.73 1054.65,-371.68 1058.71,-377.38\"/>\n</g>\n</g>\n</svg>\n"},"metadata":{}}]},{"cell_type":"code","source":"mean_absolute_error(val_y, m.predict(val_xs))","metadata":{"execution":{"iopub.status.busy":"2022-11-01T10:52:57.465774Z","iopub.execute_input":"2022-11-01T10:52:57.466101Z","iopub.status.idle":"2022-11-01T10:52:57.479718Z","shell.execute_reply.started":"2022-11-01T10:52:57.466047Z","shell.execute_reply":"2022-11-01T10:52:57.478467Z"},"trusted":true},"execution_count":71,"outputs":[{"execution_count":71,"output_type":"execute_result","data":{"text/plain":"0.18385650224215247"},"metadata":{}}]},{"cell_type":"markdown","source":"It looks like this is an improvement, although again it's a bit hard to tell with small datasets like this. Let's try submitting it to Kaggle:","metadata":{}},{"cell_type":"code","source":"tst_df[cats] = tst_df[cats].apply(lambda x: x.cat.codes)\ntst_xs,_ = xs_y(tst_df)\n\ndef subm(preds, suff):\n    tst_df['Survived'] = preds\n    sub_df = tst_df[['PassengerId','Survived']]\n    sub_df.to_csv(f'sub-{suff}.csv', index=False)\n\nsubm(m.predict(tst_xs), 'tree')","metadata":{"execution":{"iopub.status.busy":"2022-11-01T10:53:01.976049Z","iopub.execute_input":"2022-11-01T10:53:01.976309Z","iopub.status.idle":"2022-11-01T10:53:01.995469Z","shell.execute_reply.started":"2022-11-01T10:53:01.976278Z","shell.execute_reply":"2022-11-01T10:53:01.994542Z"},"trusted":true},"execution_count":72,"outputs":[]},{"cell_type":"code","source":"cats, _, tst_xs.head()","metadata":{"execution":{"iopub.status.busy":"2022-11-01T10:53:52.115299Z","iopub.execute_input":"2022-11-01T10:53:52.115947Z","iopub.status.idle":"2022-11-01T10:53:52.130605Z","shell.execute_reply.started":"2022-11-01T10:53:52.115897Z","shell.execute_reply":"2022-11-01T10:53:52.129652Z"},"trusted":true},"execution_count":77,"outputs":[{"execution_count":77,"output_type":"execute_result","data":{"text/plain":"(['Sex', 'Embarked'],\n None,\n    Sex  Embarked   Age  SibSp  Parch   LogFare  Pclass\n 0    1         1  34.5      0      0  2.178064       3\n 1    0         2  47.0      1      0  2.079442       3\n 2    1         1  62.0      0      0  2.369075       2\n 3    1         2  27.0      0      0  2.268252       3\n 4    0         2  22.0      1      1  2.586824       3)"},"metadata":{}}]},{"cell_type":"markdown","source":"When I submitted this, I got a score of 0.765, which isn't as good as our linear models or most of our neural nets, but it's pretty close to those results.\n\nHopefully you can now see why we didn't really need to create dummy variables, but instead just converted the labels into numbers using some (potentially arbitary) ordering of categories. For instance, here's how the first few items of `Embarked` are labeled:","metadata":{}},{"cell_type":"code","source":"df.Embarked.head()","metadata":{"execution":{"iopub.status.busy":"2022-11-01T10:54:08.770270Z","iopub.execute_input":"2022-11-01T10:54:08.770589Z","iopub.status.idle":"2022-11-01T10:54:08.777942Z","shell.execute_reply.started":"2022-11-01T10:54:08.770546Z","shell.execute_reply":"2022-11-01T10:54:08.777023Z"},"trusted":true},"execution_count":78,"outputs":[{"execution_count":78,"output_type":"execute_result","data":{"text/plain":"0    S\n1    C\n2    S\n3    S\n4    S\nName: Embarked, dtype: category\nCategories (3, object): ['C', 'Q', 'S']"},"metadata":{}}]},{"cell_type":"markdown","source":"...resulting in these integer codes:","metadata":{}},{"cell_type":"code","source":"df.Embarked.cat.codes.head()\n# cat.codes change categorical variables into numbers for each catergory","metadata":{"execution":{"iopub.status.busy":"2022-11-01T10:54:35.011575Z","iopub.execute_input":"2022-11-01T10:54:35.012346Z","iopub.status.idle":"2022-11-01T10:54:35.019387Z","shell.execute_reply.started":"2022-11-01T10:54:35.012285Z","shell.execute_reply":"2022-11-01T10:54:35.018638Z"},"trusted":true},"execution_count":80,"outputs":[{"execution_count":80,"output_type":"execute_result","data":{"text/plain":"0    2\n1    0\n2    2\n3    2\n4    2\ndtype: int8"},"metadata":{}}]},{"cell_type":"markdown","source":"So let's say we wanted to split into \"C\" in one group, vs \"Q\" or \"S\" in the other group. Then we just have to split on codes `<=0` (since `C` is mapped to category `0`). Note that if we wanted to split into \"Q\" in one group, we'd need to use two binary splits, first to separate \"C\" from \"Q\" and \"S\", and then a second split to separate \"Q\" from \"S\". For this reason, sometimes it can still be helpful to use dummy variables for categorical variables with few levels (like this one).\n\nIn practice, I often use dummy variables for <4 levels, and numeric codes for >=4 levels.","metadata":{}},{"cell_type":"markdown","source":"## The random forest","metadata":{}},{"cell_type":"markdown","source":"We can't make the decision tree much bigger than the example above, since some leaf nodes already have only 50 rows in them. That's not a lot of data to make a prediction.\n\nSo how could we use bigger trees? One big insight came from Leo Breiman: what if we create lots of bigger trees, and take the average of their predictions? Taking the average prediction of a bunch of models in this way is known as [bagging](https://link.springer.com/article/10.1007/BF00058655).\n\nThe idea is that we want each model's predictions in the averaged ensemble to be uncorrelated with each other model. That way, if we average the predictions, the average will be equal to the true target value -- that's because the average of lots of uncorrelated random errors is zero. That's quite an amazing insight!\n\nOne way we can create a bunch of uncorrelated models is to train each of them on a different random subset of the data. Here's how we can create a tree on a random subset of the data:","metadata":{}},{"cell_type":"code","source":"def get_tree(prop=0.75):\n    n = len(trn_y)\n    idxs = random.choice(n, int(n*prop))\n    return DecisionTreeClassifier(min_samples_leaf=5).fit(trn_xs.iloc[idxs], trn_y.iloc[idxs])","metadata":{"execution":{"iopub.status.busy":"2022-11-01T10:55:24.280460Z","iopub.execute_input":"2022-11-01T10:55:24.281064Z","iopub.status.idle":"2022-11-01T10:55:24.286354Z","shell.execute_reply.started":"2022-11-01T10:55:24.281022Z","shell.execute_reply":"2022-11-01T10:55:24.285350Z"},"trusted":true},"execution_count":84,"outputs":[]},{"cell_type":"code","source":"n = len(trn_y)\nrandom.choice(n, int(n))","metadata":{"execution":{"iopub.status.busy":"2022-11-01T10:55:25.735180Z","iopub.execute_input":"2022-11-01T10:55:25.735466Z","iopub.status.idle":"2022-11-01T10:55:25.742725Z","shell.execute_reply.started":"2022-11-01T10:55:25.735432Z","shell.execute_reply":"2022-11-01T10:55:25.742039Z"},"trusted":true},"execution_count":85,"outputs":[{"execution_count":85,"output_type":"execute_result","data":{"text/plain":"array([232,  16, 359, 222, 447, 364, 325, 466, 413, 454, 585, 232, 223, 411, 571, 334, 221,  36, 277, 195, 483, 241, 323, 191,\n        16, 523, 400,  21, 473, 119, 209, 305, 269, 284, 226, 380, 459,   6,  28,  32, 645,  78, 458, 569, 537, 481, 429, 284,\n       515,  98, 396, 488,  38, 270, 156,  74, 129,  83,  21, 458, 538, 381, 439, 603,  20, 302,  83, 658, 452, 386, 506, 256,\n       569, 526, 161, 162, 523, 270, 491, 207, 538, 583,  59, 429, 656, 604, 308, 519, 590,  26, 561, 198, 386, 348, 535, 377,\n       557, 526, 152, 205, 454, 272, 235, 520, 500, 116, 385, 343, 583, 394,  63,  71, 273, 319, 337, 365, 494, 606, 165, 543,\n        10, 300, 101, 424, 391, 213,  87, 552, 400, 587,  45, 462, 591, 309,  85,  91, 659, 241,   1, 224, 588, 115, 665, 102,\n       193, 165,  89, 121, 142, 105, 427, 304, 316, 634,   8,  72, 358, 222, 415, 552, 369, 263, 317,  83, 317, 110, 250, 382,\n         6, 458,  83, 207, 389, 313,  21, 409, 514, 126,  13,  11,  86, 523, 536, 300, 242, 121, 146, 566, 355, 630, 308, 287,\n       306, 593, 661, 548, 351, 311,  58, 642,  34, 444,  92, 206, 352, 331, 158,  23, 403, 515, 363, 381, 531, 151, 561, 607,\n       616, 419, 249, 116, 511, 577, 334, 417, 294, 370, 185, 245, 636, 267, 642, 236, 529, 551,  20, 519,  99, 477, 598, 297,\n       525, 333,   1, 374, 292, 552, 317, 501, 380, 622,  13, 503, 230,  83, 129, 144,  27,  65, 162, 228, 260, 448, 423, 459,\n       300, 302, 286, 394,   9, 629, 269, 591, 604, 360, 256, 132, 275, 512, 312, 575, 283, 120, 106, 203, 164, 304, 496, 631,\n       555, 155, 366, 619, 250, 333, 641, 510, 292,  19, 281,   0, 118,  34, 197, 484,  58, 160, 205,  11, 489, 275, 241, 215,\n       250, 125, 216, 305, 184,  38, 222, 183, 494, 426, 177, 427, 293, 150, 383, 637, 307, 335, 374,   7, 340, 128, 351, 357,\n       516, 494,  85, 174, 192, 310, 118, 217, 426, 413, 113, 144, 537, 647, 302, 587, 344,   0, 146, 286, 653,  52, 303,  68,\n       360, 601, 219, 574, 584, 160,  88, 584, 588, 158, 488, 323, 609, 380, 101, 493, 643, 613,  18, 453, 481, 593, 396, 278,\n       364, 330, 640, 343, 454, 600, 654,  64,  75, 142, 604, 472, 416, 304, 496, 290, 101, 266, 621,   2,  84, 327, 449, 453,\n       628, 220, 153, 664,  11, 132, 303, 292, 299,  42, 342, 325, 497, 363, 426,  20, 406, 618, 335, 116, 123, 398,  50, 323,\n       140, 469, 333, 372, 563, 493, 538, 155, 290, 647, 143, 105, 419, 657,   1, 588, 643,  89, 344, 349, 458, 262, 350, 107,\n       585,  38, 158, 582, 311, 240, 444, 297,  35,  83, 451, 384, 171, 375, 361, 558, 140, 117, 276, 219,  25, 520,   9, 377,\n       632, 636, 583, 151,  48, 153, 292, 579,  49, 663, 530, 611,  59, 166, 201, 145, 438, 241, 580, 560, 482, 107, 225, 630,\n       660, 661, 431, 654, 418, 454, 159, 522, 649, 589,  76, 508, 440, 253, 319, 262, 596, 657, 553, 373, 636, 166, 458, 279,\n       301, 530, 579, 217, 101, 553, 287, 166, 620,  35, 352,  72, 647, 352,  69, 424, 560, 641, 516, 427, 538, 159, 148, 229,\n       561,  66, 380,  57, 258, 341, 129, 171, 613, 208, 353, 470, 115, 263, 661, 441, 547, 452, 476, 286, 302, 310, 207, 617,\n       445, 632,  54,  11, 656, 399, 647, 514, 655, 541, 236, 278, 347, 626, 103, 629, 516, 247, 363, 320, 632, 285, 148,  83,\n       405, 455, 160, 459, 549, 499, 508, 653, 480, 662, 113, 577, 360, 340, 448, 179, 406, 665, 195, 465, 310, 261, 418, 405,\n       142, 341, 315, 100, 243, 489,  11, 238, 560,  41, 593, 428, 157, 215, 461, 133,  66,   6, 525,  72, 398, 404, 577, 129,\n       465, 205, 265, 536, 252, 288, 163,  94, 109, 335, 237, 380, 435, 482,  24, 652, 116, 481, 391, 536])"},"metadata":{}}]},{"cell_type":"markdown","source":"Now we can create as many trees as we want:","metadata":{}},{"cell_type":"code","source":"trees = [get_tree() for t in range(100)]","metadata":{"execution":{"iopub.status.busy":"2022-11-01T10:55:33.335314Z","iopub.execute_input":"2022-11-01T10:55:33.335969Z","iopub.status.idle":"2022-11-01T10:55:33.602947Z","shell.execute_reply.started":"2022-11-01T10:55:33.335908Z","shell.execute_reply":"2022-11-01T10:55:33.602148Z"},"trusted":true},"execution_count":86,"outputs":[]},{"cell_type":"code","source":"trees","metadata":{"execution":{"iopub.status.busy":"2022-11-01T10:55:52.022816Z","iopub.execute_input":"2022-11-01T10:55:52.023108Z","iopub.status.idle":"2022-11-01T10:55:52.050384Z","shell.execute_reply.started":"2022-11-01T10:55:52.023077Z","shell.execute_reply":"2022-11-01T10:55:52.049617Z"},"trusted":true},"execution_count":89,"outputs":[{"execution_count":89,"output_type":"execute_result","data":{"text/plain":"[DecisionTreeClassifier(min_samples_leaf=5),\n DecisionTreeClassifier(min_samples_leaf=5),\n DecisionTreeClassifier(min_samples_leaf=5),\n DecisionTreeClassifier(min_samples_leaf=5),\n DecisionTreeClassifier(min_samples_leaf=5),\n DecisionTreeClassifier(min_samples_leaf=5),\n DecisionTreeClassifier(min_samples_leaf=5),\n DecisionTreeClassifier(min_samples_leaf=5),\n DecisionTreeClassifier(min_samples_leaf=5),\n DecisionTreeClassifier(min_samples_leaf=5),\n DecisionTreeClassifier(min_samples_leaf=5),\n DecisionTreeClassifier(min_samples_leaf=5),\n DecisionTreeClassifier(min_samples_leaf=5),\n DecisionTreeClassifier(min_samples_leaf=5),\n DecisionTreeClassifier(min_samples_leaf=5),\n DecisionTreeClassifier(min_samples_leaf=5),\n DecisionTreeClassifier(min_samples_leaf=5),\n DecisionTreeClassifier(min_samples_leaf=5),\n DecisionTreeClassifier(min_samples_leaf=5),\n DecisionTreeClassifier(min_samples_leaf=5),\n DecisionTreeClassifier(min_samples_leaf=5),\n DecisionTreeClassifier(min_samples_leaf=5),\n DecisionTreeClassifier(min_samples_leaf=5),\n DecisionTreeClassifier(min_samples_leaf=5),\n DecisionTreeClassifier(min_samples_leaf=5),\n DecisionTreeClassifier(min_samples_leaf=5),\n DecisionTreeClassifier(min_samples_leaf=5),\n DecisionTreeClassifier(min_samples_leaf=5),\n DecisionTreeClassifier(min_samples_leaf=5),\n DecisionTreeClassifier(min_samples_leaf=5),\n DecisionTreeClassifier(min_samples_leaf=5),\n DecisionTreeClassifier(min_samples_leaf=5),\n DecisionTreeClassifier(min_samples_leaf=5),\n DecisionTreeClassifier(min_samples_leaf=5),\n DecisionTreeClassifier(min_samples_leaf=5),\n DecisionTreeClassifier(min_samples_leaf=5),\n DecisionTreeClassifier(min_samples_leaf=5),\n DecisionTreeClassifier(min_samples_leaf=5),\n DecisionTreeClassifier(min_samples_leaf=5),\n DecisionTreeClassifier(min_samples_leaf=5),\n DecisionTreeClassifier(min_samples_leaf=5),\n DecisionTreeClassifier(min_samples_leaf=5),\n DecisionTreeClassifier(min_samples_leaf=5),\n DecisionTreeClassifier(min_samples_leaf=5),\n DecisionTreeClassifier(min_samples_leaf=5),\n DecisionTreeClassifier(min_samples_leaf=5),\n DecisionTreeClassifier(min_samples_leaf=5),\n DecisionTreeClassifier(min_samples_leaf=5),\n DecisionTreeClassifier(min_samples_leaf=5),\n DecisionTreeClassifier(min_samples_leaf=5),\n DecisionTreeClassifier(min_samples_leaf=5),\n DecisionTreeClassifier(min_samples_leaf=5),\n DecisionTreeClassifier(min_samples_leaf=5),\n DecisionTreeClassifier(min_samples_leaf=5),\n DecisionTreeClassifier(min_samples_leaf=5),\n DecisionTreeClassifier(min_samples_leaf=5),\n DecisionTreeClassifier(min_samples_leaf=5),\n DecisionTreeClassifier(min_samples_leaf=5),\n DecisionTreeClassifier(min_samples_leaf=5),\n DecisionTreeClassifier(min_samples_leaf=5),\n DecisionTreeClassifier(min_samples_leaf=5),\n DecisionTreeClassifier(min_samples_leaf=5),\n DecisionTreeClassifier(min_samples_leaf=5),\n DecisionTreeClassifier(min_samples_leaf=5),\n DecisionTreeClassifier(min_samples_leaf=5),\n DecisionTreeClassifier(min_samples_leaf=5),\n DecisionTreeClassifier(min_samples_leaf=5),\n DecisionTreeClassifier(min_samples_leaf=5),\n DecisionTreeClassifier(min_samples_leaf=5),\n DecisionTreeClassifier(min_samples_leaf=5),\n DecisionTreeClassifier(min_samples_leaf=5),\n DecisionTreeClassifier(min_samples_leaf=5),\n DecisionTreeClassifier(min_samples_leaf=5),\n DecisionTreeClassifier(min_samples_leaf=5),\n DecisionTreeClassifier(min_samples_leaf=5),\n DecisionTreeClassifier(min_samples_leaf=5),\n DecisionTreeClassifier(min_samples_leaf=5),\n DecisionTreeClassifier(min_samples_leaf=5),\n DecisionTreeClassifier(min_samples_leaf=5),\n DecisionTreeClassifier(min_samples_leaf=5),\n DecisionTreeClassifier(min_samples_leaf=5),\n DecisionTreeClassifier(min_samples_leaf=5),\n DecisionTreeClassifier(min_samples_leaf=5),\n DecisionTreeClassifier(min_samples_leaf=5),\n DecisionTreeClassifier(min_samples_leaf=5),\n DecisionTreeClassifier(min_samples_leaf=5),\n DecisionTreeClassifier(min_samples_leaf=5),\n DecisionTreeClassifier(min_samples_leaf=5),\n DecisionTreeClassifier(min_samples_leaf=5),\n DecisionTreeClassifier(min_samples_leaf=5),\n DecisionTreeClassifier(min_samples_leaf=5),\n DecisionTreeClassifier(min_samples_leaf=5),\n DecisionTreeClassifier(min_samples_leaf=5),\n DecisionTreeClassifier(min_samples_leaf=5),\n DecisionTreeClassifier(min_samples_leaf=5),\n DecisionTreeClassifier(min_samples_leaf=5),\n DecisionTreeClassifier(min_samples_leaf=5),\n DecisionTreeClassifier(min_samples_leaf=5),\n DecisionTreeClassifier(min_samples_leaf=5),\n DecisionTreeClassifier(min_samples_leaf=5)]"},"metadata":{}}]},{"cell_type":"markdown","source":"Our prediction will be the average of these trees' predictions:","metadata":{}},{"cell_type":"code","source":"all_probs = [t.predict(val_xs) for t in trees]\navg_probs = np.stack(all_probs).mean(0)\n\nmean_absolute_error(val_y, avg_probs)\n\n# Average of all predictions means we used bagging","metadata":{"execution":{"iopub.status.busy":"2022-11-01T10:56:16.457400Z","iopub.execute_input":"2022-11-01T10:56:16.457986Z","iopub.status.idle":"2022-11-01T10:56:16.594673Z","shell.execute_reply.started":"2022-11-01T10:56:16.457948Z","shell.execute_reply":"2022-11-01T10:56:16.593767Z"},"trusted":true},"execution_count":91,"outputs":[{"execution_count":91,"output_type":"execute_result","data":{"text/plain":"0.2275336322869955"},"metadata":{}}]},{"cell_type":"markdown","source":"This is nearly identical to what `sklearn`'s `RandomForestClassifier` does. The main extra piece in a \"real\" random forest is that as well as choosing a random sample of data for each tree, it also picks a random subset of columns for each split. Here's how we repeat the above process with a random forest:","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\nrf = RandomForestClassifier(100, min_samples_leaf=5)\nrf.fit(trn_xs, trn_y);\nmean_absolute_error(val_y, rf.predict(val_xs))","metadata":{"execution":{"iopub.status.busy":"2022-11-01T10:56:22.606410Z","iopub.execute_input":"2022-11-01T10:56:22.606741Z","iopub.status.idle":"2022-11-01T10:56:22.874440Z","shell.execute_reply.started":"2022-11-01T10:56:22.606710Z","shell.execute_reply":"2022-11-01T10:56:22.873542Z"},"trusted":true},"execution_count":92,"outputs":[{"execution_count":92,"output_type":"execute_result","data":{"text/plain":"0.18385650224215247"},"metadata":{}}]},{"cell_type":"markdown","source":"We can submit that to Kaggle too:","metadata":{}},{"cell_type":"code","source":"subm(rf.predict(tst_xs), 'rf')","metadata":{"execution":{"iopub.status.busy":"2022-11-01T10:56:31.712479Z","iopub.execute_input":"2022-11-01T10:56:31.713006Z","iopub.status.idle":"2022-11-01T10:56:31.743308Z","shell.execute_reply.started":"2022-11-01T10:56:31.712969Z","shell.execute_reply":"2022-11-01T10:56:31.742627Z"},"trusted":true},"execution_count":93,"outputs":[]},{"cell_type":"markdown","source":"I found that gave nearly an identical result as our single tree (which, in turn, was slightly lower than our linear and neural net models in the previous notebook).","metadata":{}},{"cell_type":"markdown","source":"One particularly nice feature of random forests is they can tell us which independent variables were the most important in the model, using `feature_importances_`:","metadata":{}},{"cell_type":"code","source":"pd.DataFrame(dict(cols=trn_xs.columns, imp=m.feature_importances_)).plot('cols', 'imp', 'barh');","metadata":{"execution":{"iopub.status.busy":"2022-11-01T10:56:40.903879Z","iopub.execute_input":"2022-11-01T10:56:40.904566Z","iopub.status.idle":"2022-11-01T10:56:41.129136Z","shell.execute_reply.started":"2022-11-01T10:56:40.904491Z","shell.execute_reply":"2022-11-01T10:56:41.128365Z"},"trusted":true},"execution_count":95,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"markdown","source":"We can see that `Sex` is by far the most important predictor, with `Pclass` a distant second, and `LogFare` and `Age` behind that. In datasets with many columns, I generally recommend creating a feature importance plot as soon as possible, in order to find which columns are worth studying more closely. (Note also that we didn't really need to take the `log()` of `Fare`, since random forests only care about order, and `log()` doesn't change the order -- we only did it to make our graphs earlier easier to read.)\n\nFor details about deriving and understanding feature importances, and the many other important diagnostic tools provided by random forests, take a look at [chapter 8](https://github.com/fastai/fastbook/blob/master/08_collab.ipynb) of [our book](https://www.amazon.com/Deep-Learning-Coders-fastai-PyTorch/dp/1492045527).","metadata":{}},{"cell_type":"markdown","source":"## Conclusion","metadata":{}},{"cell_type":"markdown","source":"So what can we take away from all this?\n\nI think the first thing I'd note from this is that, clearly, more complex models aren't always better. Our \"OneR\" model, consisting of a single binary split, was nearly as good as our more complex models. Perhaps in practice a simple model like this might be much easier to use, and could be worth considering. Our random forest wasn't an improvement on the single decision tree at all.\n\nSo we should always be careful to benchmark simple models, as see if they're good enough for our needs. In practice, you will often find that simple models will have trouble providing adequate accuracy for more complex tasks, such as recommendation systems, NLP, computer vision, or multivariate time series. But there's no need to guess -- it's so easy to try a few different models, there's no reason not to give the simpler ones a go too!\n\nAnother thing I think we can take away is that random forests aren't actually that complicated at all. We were able to implement the key features of them in a notebook quite quickly. And they aren't sensitive to issues like normalization, interactions, or non-linear transformations, which make them extremely easy to work with, and hard to mess up!","metadata":{}},{"cell_type":"markdown","source":"If you found this notebook useful, please remember to click the little up-arrow at the top to upvote it, since I like to know when people have found my work useful, and it helps others find it too. (BTW, be sure you're looking at my [original notebook here](https://www.kaggle.com/jhoward/how-random-forests-work) when you do that, and are not on your own copy of it, otherwise your upvote won't get counted!) And if you have any questions or comments, please pop them below -- I read every comment I receive!","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}